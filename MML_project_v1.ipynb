{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3644bce4-9824-4c5e-81de-5b0bbd7eb39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Loaded. Train: (7973, 7), Test: (3, 2)\n",
      "\n",
      "üîÑ Processing Train Set...\n",
      "   Running Methyl Capping...\n",
      "   ‚úÖ All molecules sanitized successfully.\n",
      "   Final shape for Train Set: (7973, 8)\n",
      "\n",
      "üîÑ Processing Test Set...\n",
      "   Running Methyl Capping...\n",
      "   ‚úÖ All molecules sanitized successfully.\n",
      "   Final shape for Test Set: (3, 3)\n",
      "\n",
      "üîç Verification:\n",
      "       id                                             SMILES  \\\n",
      "0   87817                         *CC(*)c1ccccc1C(=O)OCCCCCC   \n",
      "1  106919  *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...   \n",
      "\n",
      "                                    Sanitized_SMILES  \n",
      "0                         CCCCCCOC(=O)c1ccccc1C(C)CC  \n",
      "1  CCCCCC1CCC(c2ccc([C@@H](CCC)c3ccc(NC)cc3)cc2)(...  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SEGMENT 1: Environment Setup & Chemical Sanitization\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# RDKit libraries\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Suppress warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define file paths (Update these paths if running on a different machine)\n",
    "TRAIN_PATH = \"C:/Users/Benjamin Gu/Desktop/MML_project/train.csv\"\n",
    "TEST_PATH = \"C:/Users/Benjamin Gu/Desktop/MML_project/test.csv\"\n",
    "\n",
    "def load_data(train_path, test_path):\n",
    "   if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "       raise FileNotFoundError(f\"Files not found: {train_path} or {test_path}\")\n",
    "   \n",
    "   train_df = pd.read_csv(train_path)\n",
    "   test_df = pd.read_csv(test_path)\n",
    "   print(f\"‚úÖ Data Loaded. Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "   return train_df, test_df\n",
    "\n",
    "def sanitize_polymer_smiles(smiles, capping_group='C'):\n",
    "   # Type check to prevent errors with NaN or non-string inputs\n",
    "   if pd.isna(smiles) or not isinstance(smiles, str):\n",
    "       return None\n",
    "   \n",
    "   # Methyl Capping: Replace wildcard '*' with 'C'\n",
    "   sanitized_smiles = smiles.replace('*', capping_group)\n",
    "   try:\n",
    "       mol = Chem.MolFromSmiles(sanitized_smiles)\n",
    "       if mol:\n",
    "           # Return standardized Canonical SMILES\n",
    "           return Chem.MolToSmiles(mol, canonical=True)\n",
    "       return None\n",
    "   except:\n",
    "       return None\n",
    "\n",
    "def preprocess_pipeline(df, name=\"Dataset\"):\n",
    "   print(f\"\\nüîÑ Processing {name}...\")\n",
    "   df_clean = df.copy()\n",
    "   \n",
    "   # Check if SMILES column exists\n",
    "   target_col = 'SMILES'\n",
    "   if target_col not in df_clean.columns:\n",
    "       raise KeyError(f\"Column '{target_col}' not found in {name}. Please check CSV headers.\")\n",
    "\n",
    "   # 1. Apply sanitization ONLY to the SMILES column\n",
    "   print(\"   Running Methyl Capping...\")\n",
    "   df_clean['Sanitized_SMILES'] = df_clean[target_col].apply(\n",
    "       lambda x: sanitize_polymer_smiles(x, capping_group='C')\n",
    "   )\n",
    "   \n",
    "   # 2. Check for failures\n",
    "   n_failures = df_clean['Sanitized_SMILES'].isna().sum()\n",
    "   \n",
    "   if n_failures > 0:\n",
    "       print(f\"   ‚ö†Ô∏è Warning: {n_failures} molecules failed sanitization and will be dropped.\")\n",
    "       df_clean = df_clean.dropna(subset=['Sanitized_SMILES']).reset_index(drop=True)\n",
    "   else:\n",
    "       print(f\"   ‚úÖ All molecules sanitized successfully.\")\n",
    "       \n",
    "   print(f\"   Final shape for {name}: {df_clean.shape}\")\n",
    "   return df_clean\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "   # Load Data\n",
    "   raw_train, raw_test = load_data(TRAIN_PATH, TEST_PATH)\n",
    "   \n",
    "   # Run Preprocessing Pipeline\n",
    "   clean_train = preprocess_pipeline(raw_train, name=\"Train Set\")\n",
    "   clean_test = preprocess_pipeline(raw_test, name=\"Test Set\")\n",
    "\n",
    "   print(\"\\nüîç Verification:\")\n",
    "   # Display first two rows for verification\n",
    "   cols_to_show = ['id', 'SMILES', 'Sanitized_SMILES']\n",
    "   if 'id' in clean_train.columns:\n",
    "       print(clean_train[cols_to_show].head(2))\n",
    "   else:\n",
    "       print(clean_train[['SMILES', 'Sanitized_SMILES']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed97fdf6-015e-4a74-88b8-2995f58b2617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üñê Generating Morgan Fingerprints...\n",
      "  üñê Generating Morgan Fingerprints...\n",
      "   ‚öóÔ∏è Calculating RDKit descriptors...\n",
      "   ‚öóÔ∏è Calculating RDKit descriptors...\n",
      "   (Note: Running on full dataset may take time on CPU)\n",
      "   ü§ñ Extracting Transformer Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:01<00:00, 200.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ü§ñ Extracting Transformer Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 285.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature Engineering Done. Full Train Shape: (7973, 2446)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SEGMENT 2: Hybrid Feature Engineering\n",
    "# =============================================================================\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_rdkit_features(smiles):\n",
    "   mol = Chem.MolFromSmiles(smiles)\n",
    "   if not mol: return {}\n",
    "   return {\n",
    "       'MolLogP': Descriptors.MolLogP(mol),\n",
    "       'TPSA': Descriptors.TPSA(mol),\n",
    "       'MolWt': Descriptors.MolWt(mol),\n",
    "       'BertzCT': Descriptors.BertzCT(mol),\n",
    "       'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
    "       'RingCount': Descriptors.RingCount(mol)\n",
    "   }\n",
    "\n",
    "def process_rdkit_features(df):\n",
    "   print(\"   ‚öóÔ∏è Calculating RDKit descriptors...\")\n",
    "   \n",
    "   # Calculate features for every SMILES in the list\n",
    "   features_list = [calculate_rdkit_features(x) for x in df['Sanitized_SMILES']]\n",
    "   \n",
    "   # Convert list of dictionaries to DataFrame\n",
    "   feat_df = pd.DataFrame(features_list)\n",
    "   \n",
    "   # Concatenate original data with new features\n",
    "   return pd.concat([df.reset_index(drop=True), feat_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "def get_chemberta_embeddings(smiles_list):\n",
    "   print(\"   ü§ñ Extracting Transformer Embeddings...\")\n",
    "   model_name = 'DeepChem/ChemBERTa-77M-MLM'\n",
    "   \n",
    "   # Load Model\n",
    "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "   model = AutoModel.from_pretrained(model_name)\n",
    "   \n",
    "   # Check for GPU\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   model.to(device)\n",
    "   print(f\"      Running on: {device}\")\n",
    "   \n",
    "   batch_size = 32\n",
    "   all_embeddings = []\n",
    "   \n",
    "   for i in tqdm(range(0, len(smiles_list), batch_size)):\n",
    "       batch = smiles_list[i:i+batch_size]\n",
    "       \n",
    "       # Tokenize\n",
    "       inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
    "       \n",
    "       with torch.no_grad():\n",
    "           outputs = model(**inputs)\n",
    "       \n",
    "       # Mean pooling (average over tokens to get sentence embedding)\n",
    "       embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "       all_embeddings.append(embeddings)\n",
    "       \n",
    "   return np.vstack(all_embeddings)\n",
    "\n",
    "def get_morgan_fingerprints(df, n_bits=2048):\n",
    "   print(\"  üñê Generating Morgan Fingerprints...\")\n",
    "   \n",
    "   # Helper function: Convert fingerprint to numpy array\n",
    "   def _get_fp(smiles):\n",
    "       mol = Chem.MolFromSmiles(smiles)\n",
    "       if not mol: return np.zeros((n_bits,))\\\n",
    "       fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
    "       arr = np.zeros((0,), dtype=np.int8)\n",
    "       from rdkit.DataStructs import ConvertToNumpyArray\n",
    "       ConvertToNumpyArray(fp, arr)\n",
    "       return arr\n",
    "       \n",
    "   # Calculate fingerprints\n",
    "   fps = np.stack(df['Sanitized_SMILES'].apply(_get_fp).values)\n",
    "   \n",
    "   # Convert to DataFrame\n",
    "   col_names = [f'FP_{i}' for i in range(n_bits)]\n",
    "   return pd.DataFrame(fps, columns=col_names)\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "# 1. Generate Morgan Fingerprints\n",
    "train_fp = get_morgan_fingerprints(clean_train)\n",
    "test_fp = get_morgan_fingerprints(clean_test)\n",
    "\n",
    "# 2. RDKit Features\n",
    "clean_train = process_rdkit_features(clean_train)\n",
    "clean_test = process_rdkit_features(clean_test)\n",
    "\n",
    "# 3. ChemBERTa Embeddings\n",
    "print(\"   (Note: Running on full dataset may take time on CPU)\")\n",
    "\n",
    "# Convert column to list for tokenizer\n",
    "train_smiles = clean_train['Sanitized_SMILES'].tolist()\n",
    "test_smiles = clean_test['Sanitized_SMILES'].tolist()\n",
    "\n",
    "train_emb = get_chemberta_embeddings(train_smiles)\n",
    "test_emb = get_chemberta_embeddings(test_smiles)\n",
    "\n",
    "# Convert embeddings to DataFrame with auto-generated column names\n",
    "emb_cols = [f'ChemBERTa_{i}' for i in range(train_emb.shape[1])]\n",
    "train_emb_df = pd.DataFrame(train_emb, columns=emb_cols)\n",
    "test_emb_df = pd.DataFrame(test_emb, columns=emb_cols)\n",
    "\n",
    "# 4. Concatenate all features\n",
    "full_train = pd.concat([clean_train.reset_index(drop=True), train_emb_df, train_fp], axis=1)\n",
    "full_test = pd.concat([clean_test.reset_index(drop=True), test_emb_df, test_fp], axis=1)\n",
    "\n",
    "print(f\"‚úÖ Feature Engineering Done. Full Train Shape: {full_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69e6b83-e72d-4fd2-9051-7e6bd7a96dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß¨ Generating Scaffolds...\n",
      "   Unique Scaffolds in Train: 3072\n",
      "                                       Sanitized_SMILES  \\\n",
      "0                            CCCCCCOC(=O)c1ccccc1C(C)CC   \n",
      "1     CCCCCC1CCC(c2ccc([C@@H](CCC)c3ccc(NC)cc3)cc2)(...   \n",
      "2     COc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...   \n",
      "3     CNc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(NC)...   \n",
      "4     COc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...   \n",
      "...                                                 ...   \n",
      "7968       CCCCCCCCc1cc(OC)cc(OC(=O)c2cccc(C(C)=O)c2)c1   \n",
      "7969  CC(=O)OCCN(CCOC(=O)c1ccc2c(c1)C(=O)N(c1cccc(N3...   \n",
      "7970  CCCCCCCCNC(=O)c1cc(C)cc(N2C(=O)c3ccc(-c4ccc5c(...   \n",
      "7971                                   CC=C(C)c1ccccc1C   \n",
      "7972  Cc1ccc(OCCCCCCCCCCCOC(=O)CCCCC(=O)OCCCCCCCCCCC...   \n",
      "\n",
      "                                               Scaffold  \n",
      "0                                              c1ccccc1  \n",
      "1     c1ccc(Cc2ccc(C3(c4ccc(Cc5ccccc5)cc4)CCCCC3)cc2...  \n",
      "2     O=C1C(=Cc2ccccc2)CCCC1=Cc1ccc(Oc2ccc(S(=O)(=O)...  \n",
      "3     c1ccc(-c2cc(-c3ccccc3)c(-c3ccccc3)c(-c3ccccc3)...  \n",
      "4     O=C(Oc1ccccc1)c1cc(OCCCCCCCCCOCC2CCCN2c2ccccc2...  \n",
      "...                                                 ...  \n",
      "7968                             O=C(Oc1ccccc1)c1ccccc1  \n",
      "7969  O=C(OCCNc1ccc(N=Nc2ccccc2)cc1)c1ccc2c(c1)C(=O)...  \n",
      "7970  O=C1NC(=O)c2cc(-c3ccc4c(c3)C(=O)N(c3ccccc3)C4=...  \n",
      "7971                                           c1ccccc1  \n",
      "7972  O=C(CCCCC(=O)OCCCCCCCCCCCOc1ccc(-c2nncs2)cc1)O...  \n",
      "\n",
      "[7973 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SEGMENT 3: Scaffold Splitting\n",
    "# =============================================================================\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit import Chem\n",
    "\n",
    "def get_scaffold(smiles):\n",
    "   # Type check to prevent errors\n",
    "   if not isinstance(smiles, str):\n",
    "       return 'Generic'\n",
    "   \n",
    "   try:\n",
    "       mol = Chem.MolFromSmiles(smiles)\n",
    "       if mol:\n",
    "           scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "           return Chem.MolToSmiles(scaffold)\n",
    "       return 'Generic'\n",
    "   except:\n",
    "       return 'Generic'\n",
    "\n",
    "print(\"\\nüß¨ Generating Scaffolds...\")\n",
    "\n",
    "# Apply to 'Sanitized_SMILES' column and save to new 'Scaffold' column\n",
    "full_train['Scaffold'] = full_train['Sanitized_SMILES'].apply(get_scaffold)\n",
    "\n",
    "# Apply to test set as well (useful for post-analysis)\n",
    "if 'Sanitized_SMILES' in full_test.columns:\n",
    "   full_test['Scaffold'] = full_test['Sanitized_SMILES'].apply(get_scaffold)\n",
    "\n",
    "print(f\"   Unique Scaffolds in Train: {full_train['Scaffold'].nunique()}\")\n",
    "\n",
    "# Verify the Scaffold column generation\n",
    "print(full_train[['Sanitized_SMILES', 'Scaffold']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82f109c-5b54-4c9f-ae01-094438526ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.9.23\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       16.50 GB / 31.11 GB (53.0%)\n",
      "Disk Space Avail:   14.74 GB / 1862.21 GB (0.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 45s of the 180s of remaining time (25%).\n",
      "\t\tContext path: \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tg\\ds_sub_fit\\sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Training for Target: Tg | ‚è≥ Time Limit: 180s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 45s\n",
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tg\\ds_sub_fit\\sub_fit_ho\"\n",
      "Train Data Rows:    454\n",
      "Train Data Columns: 2438\n",
      "Label Column:       Tg\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16893.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.57 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1511 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 537): ['FP_0', 'FP_12', 'FP_15', 'FP_17', 'FP_22', 'FP_28', 'FP_30', 'FP_32', 'FP_36', 'FP_40', 'FP_43', 'FP_48', 'FP_51', 'FP_55', 'FP_60', 'FP_64', 'FP_77', 'FP_78', 'FP_82', 'FP_89', 'FP_93', 'FP_95', 'FP_100', 'FP_111', 'FP_112', 'FP_113', 'FP_122', 'FP_124', 'FP_129', 'FP_132', 'FP_134', 'FP_137', 'FP_138', 'FP_143', 'FP_148', 'FP_149', 'FP_150', 'FP_153', 'FP_154', 'FP_156', 'FP_157', 'FP_160', 'FP_161', 'FP_164', 'FP_171', 'FP_173', 'FP_174', 'FP_176', 'FP_178', 'FP_185', 'FP_188', 'FP_196', 'FP_198', 'FP_200', 'FP_201', 'FP_211', 'FP_214', 'FP_217', 'FP_218', 'FP_219', 'FP_225', 'FP_228', 'FP_234', 'FP_239', 'FP_248', 'FP_253', 'FP_256', 'FP_259', 'FP_261', 'FP_269', 'FP_271', 'FP_275', 'FP_284', 'FP_286', 'FP_297', 'FP_299', 'FP_302', 'FP_303', 'FP_306', 'FP_308', 'FP_320', 'FP_321', 'FP_326', 'FP_332', 'FP_336', 'FP_337', 'FP_338', 'FP_344', 'FP_346', 'FP_347', 'FP_349', 'FP_359', 'FP_362', 'FP_365', 'FP_367', 'FP_368', 'FP_369', 'FP_371', 'FP_382', 'FP_390', 'FP_393', 'FP_396', 'FP_397', 'FP_398', 'FP_400', 'FP_402', 'FP_408', 'FP_415', 'FP_416', 'FP_417', 'FP_418', 'FP_431', 'FP_432', 'FP_433', 'FP_434', 'FP_435', 'FP_444', 'FP_450', 'FP_452', 'FP_453', 'FP_454', 'FP_457', 'FP_467', 'FP_468', 'FP_470', 'FP_477', 'FP_478', 'FP_483', 'FP_488', 'FP_494', 'FP_496', 'FP_500', 'FP_509', 'FP_513', 'FP_515', 'FP_516', 'FP_520', 'FP_522', 'FP_525', 'FP_533', 'FP_536', 'FP_537', 'FP_542', 'FP_543', 'FP_544', 'FP_545', 'FP_550', 'FP_558', 'FP_559', 'FP_560', 'FP_565', 'FP_566', 'FP_569', 'FP_571', 'FP_572', 'FP_576', 'FP_578', 'FP_583', 'FP_585', 'FP_590', 'FP_593', 'FP_595', 'FP_596', 'FP_604', 'FP_605', 'FP_611', 'FP_615', 'FP_620', 'FP_625', 'FP_627', 'FP_628', 'FP_633', 'FP_639', 'FP_647', 'FP_648', 'FP_649', 'FP_651', 'FP_653', 'FP_655', 'FP_657', 'FP_659', 'FP_663', 'FP_685', 'FP_686', 'FP_687', 'FP_688', 'FP_696', 'FP_700', 'FP_705', 'FP_706', 'FP_709', 'FP_711', 'FP_714', 'FP_720', 'FP_723', 'FP_727', 'FP_733', 'FP_735', 'FP_737', 'FP_740', 'FP_744', 'FP_746', 'FP_748', 'FP_752', 'FP_756', 'FP_763', 'FP_764', 'FP_765', 'FP_770', 'FP_771', 'FP_776', 'FP_778', 'FP_779', 'FP_780', 'FP_784', 'FP_787', 'FP_788', 'FP_793', 'FP_796', 'FP_804', 'FP_805', 'FP_810', 'FP_812', 'FP_818', 'FP_821', 'FP_827', 'FP_830', 'FP_834', 'FP_842', 'FP_848', 'FP_858', 'FP_860', 'FP_865', 'FP_882', 'FP_887', 'FP_889', 'FP_894', 'FP_904', 'FP_905', 'FP_910', 'FP_911', 'FP_912', 'FP_921', 'FP_922', 'FP_928', 'FP_948', 'FP_950', 'FP_952', 'FP_955', 'FP_964', 'FP_966', 'FP_971', 'FP_981', 'FP_983', 'FP_990', 'FP_991', 'FP_995', 'FP_996', 'FP_998', 'FP_1000', 'FP_1006', 'FP_1009', 'FP_1016', 'FP_1018', 'FP_1023', 'FP_1026', 'FP_1037', 'FP_1040', 'FP_1042', 'FP_1046', 'FP_1048', 'FP_1050', 'FP_1054', 'FP_1062', 'FP_1067', 'FP_1068', 'FP_1072', 'FP_1085', 'FP_1090', 'FP_1093', 'FP_1094', 'FP_1098', 'FP_1099', 'FP_1102', 'FP_1105', 'FP_1110', 'FP_1111', 'FP_1112', 'FP_1117', 'FP_1137', 'FP_1141', 'FP_1144', 'FP_1156', 'FP_1161', 'FP_1167', 'FP_1173', 'FP_1175', 'FP_1177', 'FP_1186', 'FP_1187', 'FP_1190', 'FP_1193', 'FP_1194', 'FP_1197', 'FP_1204', 'FP_1205', 'FP_1208', 'FP_1211', 'FP_1213', 'FP_1215', 'FP_1217', 'FP_1219', 'FP_1226', 'FP_1233', 'FP_1234', 'FP_1235', 'FP_1242', 'FP_1247', 'FP_1250', 'FP_1251', 'FP_1253', 'FP_1254', 'FP_1258', 'FP_1259', 'FP_1261', 'FP_1262', 'FP_1270', 'FP_1271', 'FP_1272', 'FP_1277', 'FP_1278', 'FP_1289', 'FP_1293', 'FP_1294', 'FP_1296', 'FP_1301', 'FP_1303', 'FP_1304', 'FP_1305', 'FP_1307', 'FP_1311', 'FP_1315', 'FP_1320', 'FP_1324', 'FP_1331', 'FP_1333', 'FP_1334', 'FP_1338', 'FP_1341', 'FP_1343', 'FP_1346', 'FP_1348', 'FP_1355', 'FP_1360', 'FP_1361', 'FP_1363', 'FP_1364', 'FP_1367', 'FP_1368', 'FP_1372', 'FP_1375', 'FP_1377', 'FP_1378', 'FP_1381', 'FP_1383', 'FP_1388', 'FP_1389', 'FP_1395', 'FP_1404', 'FP_1405', 'FP_1407', 'FP_1409', 'FP_1425', 'FP_1426', 'FP_1428', 'FP_1432', 'FP_1435', 'FP_1437', 'FP_1441', 'FP_1443', 'FP_1448', 'FP_1450', 'FP_1455', 'FP_1459', 'FP_1462', 'FP_1466', 'FP_1467', 'FP_1468', 'FP_1469', 'FP_1471', 'FP_1475', 'FP_1483', 'FP_1484', 'FP_1491', 'FP_1493', 'FP_1494', 'FP_1496', 'FP_1505', 'FP_1512', 'FP_1515', 'FP_1516', 'FP_1517', 'FP_1525', 'FP_1528', 'FP_1529', 'FP_1530', 'FP_1531', 'FP_1537', 'FP_1538', 'FP_1541', 'FP_1545', 'FP_1547', 'FP_1548', 'FP_1551', 'FP_1553', 'FP_1554', 'FP_1558', 'FP_1562', 'FP_1566', 'FP_1569', 'FP_1572', 'FP_1576', 'FP_1580', 'FP_1585', 'FP_1586', 'FP_1594', 'FP_1595', 'FP_1600', 'FP_1603', 'FP_1605', 'FP_1606', 'FP_1614', 'FP_1616', 'FP_1621', 'FP_1622', 'FP_1628', 'FP_1635', 'FP_1637', 'FP_1639', 'FP_1642', 'FP_1651', 'FP_1662', 'FP_1674', 'FP_1675', 'FP_1679', 'FP_1681', 'FP_1682', 'FP_1689', 'FP_1694', 'FP_1699', 'FP_1701', 'FP_1702', 'FP_1703', 'FP_1705', 'FP_1708', 'FP_1712', 'FP_1732', 'FP_1736', 'FP_1739', 'FP_1740', 'FP_1743', 'FP_1748', 'FP_1753', 'FP_1757', 'FP_1759', 'FP_1762', 'FP_1767', 'FP_1769', 'FP_1772', 'FP_1776', 'FP_1778', 'FP_1797', 'FP_1798', 'FP_1800', 'FP_1801', 'FP_1804', 'FP_1805', 'FP_1813', 'FP_1815', 'FP_1817', 'FP_1819', 'FP_1821', 'FP_1829', 'FP_1830', 'FP_1833', 'FP_1836', 'FP_1838', 'FP_1841', 'FP_1850', 'FP_1852', 'FP_1868', 'FP_1869', 'FP_1872', 'FP_1874', 'FP_1888', 'FP_1889', 'FP_1890', 'FP_1893', 'FP_1894', 'FP_1895', 'FP_1896', 'FP_1897', 'FP_1898', 'FP_1900', 'FP_1901', 'FP_1903', 'FP_1908', 'FP_1912', 'FP_1913', 'FP_1919', 'FP_1927', 'FP_1929', 'FP_1930', 'FP_1931', 'FP_1933', 'FP_1937', 'FP_1938', 'FP_1944', 'FP_1945', 'FP_1947', 'FP_1950', 'FP_1952', 'FP_1955', 'FP_1958', 'FP_1961', 'FP_1965', 'FP_1968', 'FP_1977', 'FP_1979', 'FP_2011', 'FP_2016', 'FP_2025', 'FP_2029', 'FP_2035', 'FP_2036', 'FP_2038', 'FP_2046']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 213): ['FP_47', 'FP_163', 'FP_209', 'FP_221', 'FP_246', 'FP_276', 'FP_277', 'FP_331', 'FP_370', 'FP_383', 'FP_385', 'FP_421', 'FP_423', 'FP_424', 'FP_437', 'FP_449', 'FP_455', 'FP_482', 'FP_510', 'FP_511', 'FP_518', 'FP_523', 'FP_529', 'FP_532', 'FP_539', 'FP_574', 'FP_577', 'FP_598', 'FP_614', 'FP_622', 'FP_662', 'FP_668', 'FP_678', 'FP_683', 'FP_724', 'FP_730', 'FP_743', 'FP_767', 'FP_773', 'FP_786', 'FP_815', 'FP_816', 'FP_823', 'FP_836', 'FP_839', 'FP_851', 'FP_853', 'FP_854', 'FP_855', 'FP_856', 'FP_874', 'FP_877', 'FP_897', 'FP_900', 'FP_938', 'FP_941', 'FP_962', 'FP_968', 'FP_969', 'FP_970', 'FP_982', 'FP_985', 'FP_993', 'FP_997', 'FP_999', 'FP_1002', 'FP_1014', 'FP_1041', 'FP_1059', 'FP_1070', 'FP_1073', 'FP_1078', 'FP_1091', 'FP_1092', 'FP_1108', 'FP_1115', 'FP_1124', 'FP_1127', 'FP_1131', 'FP_1135', 'FP_1140', 'FP_1146', 'FP_1153', 'FP_1158', 'FP_1159', 'FP_1165', 'FP_1178', 'FP_1189', 'FP_1191', 'FP_1198', 'FP_1206', 'FP_1207', 'FP_1222', 'FP_1228', 'FP_1230', 'FP_1231', 'FP_1241', 'FP_1246', 'FP_1265', 'FP_1288', 'FP_1302', 'FP_1316', 'FP_1330', 'FP_1342', 'FP_1352', 'FP_1354', 'FP_1374', 'FP_1376', 'FP_1390', 'FP_1392', 'FP_1396', 'FP_1397', 'FP_1401', 'FP_1412', 'FP_1416', 'FP_1419', 'FP_1420', 'FP_1424', 'FP_1433', 'FP_1434', 'FP_1439', 'FP_1456', 'FP_1473', 'FP_1481', 'FP_1489', 'FP_1511', 'FP_1519', 'FP_1540', 'FP_1560', 'FP_1568', 'FP_1570', 'FP_1579', 'FP_1582', 'FP_1590', 'FP_1591', 'FP_1596', 'FP_1597', 'FP_1608', 'FP_1610', 'FP_1618', 'FP_1629', 'FP_1640', 'FP_1641', 'FP_1643', 'FP_1646', 'FP_1648', 'FP_1652', 'FP_1660', 'FP_1663', 'FP_1670', 'FP_1677', 'FP_1684', 'FP_1685', 'FP_1690', 'FP_1695', 'FP_1709', 'FP_1711', 'FP_1735', 'FP_1737', 'FP_1751', 'FP_1752', 'FP_1765', 'FP_1780', 'FP_1783', 'FP_1786', 'FP_1790', 'FP_1791', 'FP_1792', 'FP_1793', 'FP_1811', 'FP_1822', 'FP_1826', 'FP_1835', 'FP_1848', 'FP_1851', 'FP_1857', 'FP_1858', 'FP_1863', 'FP_1864', 'FP_1870', 'FP_1871', 'FP_1875', 'FP_1878', 'FP_1879', 'FP_1881', 'FP_1884', 'FP_1887', 'FP_1891', 'FP_1892', 'FP_1899', 'FP_1902', 'FP_1910', 'FP_1914', 'FP_1925', 'FP_1932', 'FP_1946', 'FP_1956', 'FP_1988', 'FP_1994', 'FP_2003', 'FP_2007', 'FP_2008', 'FP_2010', 'FP_2014', 'FP_2015', 'FP_2028', 'FP_2030', 'FP_2031', 'FP_2037', 'FP_2039', 'FP_2040', 'FP_2043', 'FP_2047']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 213 | ['FP_47', 'FP_163', 'FP_209', 'FP_221', 'FP_246', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 1300 | ['NumRotatableBonds', 'RingCount', 'FP_1', 'FP_2', 'FP_3', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :    2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 1298 | ['FP_1', 'FP_2', 'FP_3', 'FP_4', 'FP_5', ...]\n",
      "\t2.4s = Fit runtime\n",
      "\t1688 features in original data used to generate 1688 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.48s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 28.34s of the 42.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-66.5626\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.13s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 16.94s of the 31.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-69.0417\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.19s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 4.44s of the 18.61s of remaining time.\n",
      "\t-70.0829\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1.87s of the 16.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1.34s of the 15.52s of remaining time.\n",
      "\t-68.3466\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.08s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 0.02s of the 14.20s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.1s)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 42.52s of the 13.81s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.783, 'ExtraTreesMSE_BAG_L1': 0.217}\n",
      "\t-66.4128\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 13.80s of the 13.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 828. Best iteration is:\n",
      "\t[792]\tvalid_set's rmse: 62.7808\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-66.4811\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.43s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2.10s of the 2.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's rmse: 94.5786\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1.23s of the 1.21s of remaining time.\n",
      "\t-68.2973\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.33s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 42.52s of the -1.48s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.696, 'ExtraTreesMSE_BAG_L1': 0.174, 'RandomForestMSE_BAG_L2': 0.13}\n",
      "\t-66.2839\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 46.5s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 437.0 rows/s (57 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tg\\ds_sub_fit\\sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  RandomForestMSE_BAG_L1     -65.013761 -70.082919  root_mean_squared_error        0.058501       0.104514   2.253723                 0.058501                0.104514           2.253723            1       True          3\n",
      "1       LightGBMXT_BAG_L1     -65.093222 -66.562636  root_mean_squared_error        0.099791       0.042513  11.125425                 0.099791                0.042513          11.125425            1       True          1\n",
      "2         LightGBM_BAG_L1     -65.145567 -69.041739  root_mean_squared_error        0.096505       0.044020  12.193061                 0.096505                0.044020          12.193061            1       True          2\n",
      "3     WeightedEnsemble_L2     -65.752758 -66.412752  root_mean_squared_error        0.169524       0.147024  12.212438                 0.007500                0.000000           0.003000            2       True          5\n",
      "4  RandomForestMSE_BAG_L2     -66.380797 -68.297301  root_mean_squared_error        0.296534       0.356544  16.793943                 0.076009                0.105006           2.330781            2       True          7\n",
      "5     WeightedEnsemble_L3     -66.493954 -66.283911  root_mean_squared_error        0.407544       0.405044  28.224363                 0.008999                0.000000           0.004002            3       True          8\n",
      "6       LightGBMXT_BAG_L2     -66.501528 -66.481070  root_mean_squared_error        0.322537       0.300038  25.889580                 0.102011                0.048500          11.426418            2       True          6\n",
      "7    ExtraTreesMSE_BAG_L1     -69.077928 -68.346623  root_mean_squared_error        0.062234       0.104511   1.084013                 0.062234                0.104511           1.084013            1       True          4\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t47s\t = DyStack   runtime |\t133s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 133s\n",
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tg\"\n",
      "Train Data Rows:    511\n",
      "Train Data Columns: 2438\n",
      "Label Column:       Tg\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16952.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.77 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1579 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 469): ['FP_0', 'FP_12', 'FP_15', 'FP_22', 'FP_28', 'FP_30', 'FP_32', 'FP_36', 'FP_40', 'FP_43', 'FP_48', 'FP_51', 'FP_55', 'FP_60', 'FP_64', 'FP_77', 'FP_78', 'FP_82', 'FP_89', 'FP_93', 'FP_95', 'FP_100', 'FP_111', 'FP_112', 'FP_113', 'FP_124', 'FP_129', 'FP_134', 'FP_137', 'FP_138', 'FP_143', 'FP_148', 'FP_149', 'FP_150', 'FP_153', 'FP_154', 'FP_156', 'FP_157', 'FP_160', 'FP_161', 'FP_164', 'FP_173', 'FP_174', 'FP_176', 'FP_178', 'FP_185', 'FP_188', 'FP_196', 'FP_198', 'FP_200', 'FP_211', 'FP_214', 'FP_217', 'FP_219', 'FP_225', 'FP_228', 'FP_234', 'FP_248', 'FP_253', 'FP_256', 'FP_259', 'FP_261', 'FP_269', 'FP_271', 'FP_275', 'FP_284', 'FP_297', 'FP_299', 'FP_303', 'FP_306', 'FP_308', 'FP_320', 'FP_321', 'FP_326', 'FP_336', 'FP_337', 'FP_338', 'FP_344', 'FP_347', 'FP_349', 'FP_362', 'FP_367', 'FP_368', 'FP_369', 'FP_371', 'FP_382', 'FP_390', 'FP_393', 'FP_397', 'FP_398', 'FP_400', 'FP_402', 'FP_408', 'FP_415', 'FP_416', 'FP_417', 'FP_418', 'FP_431', 'FP_433', 'FP_434', 'FP_444', 'FP_452', 'FP_453', 'FP_457', 'FP_467', 'FP_468', 'FP_470', 'FP_477', 'FP_478', 'FP_483', 'FP_488', 'FP_494', 'FP_496', 'FP_500', 'FP_509', 'FP_513', 'FP_515', 'FP_516', 'FP_520', 'FP_522', 'FP_525', 'FP_533', 'FP_537', 'FP_545', 'FP_550', 'FP_558', 'FP_559', 'FP_565', 'FP_566', 'FP_569', 'FP_571', 'FP_572', 'FP_576', 'FP_578', 'FP_583', 'FP_585', 'FP_590', 'FP_593', 'FP_595', 'FP_596', 'FP_604', 'FP_611', 'FP_615', 'FP_620', 'FP_625', 'FP_627', 'FP_628', 'FP_633', 'FP_639', 'FP_647', 'FP_648', 'FP_649', 'FP_651', 'FP_653', 'FP_655', 'FP_657', 'FP_659', 'FP_663', 'FP_688', 'FP_696', 'FP_700', 'FP_705', 'FP_706', 'FP_709', 'FP_711', 'FP_714', 'FP_720', 'FP_723', 'FP_727', 'FP_733', 'FP_735', 'FP_740', 'FP_744', 'FP_748', 'FP_752', 'FP_756', 'FP_763', 'FP_764', 'FP_765', 'FP_770', 'FP_771', 'FP_776', 'FP_778', 'FP_779', 'FP_780', 'FP_784', 'FP_787', 'FP_788', 'FP_793', 'FP_796', 'FP_804', 'FP_805', 'FP_810', 'FP_812', 'FP_818', 'FP_821', 'FP_827', 'FP_830', 'FP_834', 'FP_848', 'FP_858', 'FP_860', 'FP_865', 'FP_882', 'FP_887', 'FP_889', 'FP_894', 'FP_904', 'FP_905', 'FP_910', 'FP_911', 'FP_912', 'FP_921', 'FP_922', 'FP_928', 'FP_948', 'FP_950', 'FP_952', 'FP_955', 'FP_964', 'FP_966', 'FP_971', 'FP_981', 'FP_983', 'FP_990', 'FP_991', 'FP_995', 'FP_998', 'FP_1000', 'FP_1006', 'FP_1016', 'FP_1018', 'FP_1023', 'FP_1040', 'FP_1042', 'FP_1046', 'FP_1048', 'FP_1054', 'FP_1062', 'FP_1067', 'FP_1068', 'FP_1072', 'FP_1090', 'FP_1093', 'FP_1094', 'FP_1098', 'FP_1099', 'FP_1102', 'FP_1105', 'FP_1111', 'FP_1112', 'FP_1117', 'FP_1137', 'FP_1144', 'FP_1156', 'FP_1161', 'FP_1167', 'FP_1175', 'FP_1177', 'FP_1186', 'FP_1187', 'FP_1190', 'FP_1193', 'FP_1194', 'FP_1197', 'FP_1204', 'FP_1205', 'FP_1208', 'FP_1211', 'FP_1213', 'FP_1215', 'FP_1217', 'FP_1219', 'FP_1233', 'FP_1235', 'FP_1242', 'FP_1247', 'FP_1250', 'FP_1251', 'FP_1253', 'FP_1254', 'FP_1258', 'FP_1259', 'FP_1261', 'FP_1262', 'FP_1270', 'FP_1271', 'FP_1277', 'FP_1278', 'FP_1289', 'FP_1293', 'FP_1296', 'FP_1303', 'FP_1304', 'FP_1305', 'FP_1307', 'FP_1311', 'FP_1315', 'FP_1320', 'FP_1324', 'FP_1331', 'FP_1333', 'FP_1334', 'FP_1338', 'FP_1341', 'FP_1343', 'FP_1355', 'FP_1360', 'FP_1361', 'FP_1364', 'FP_1367', 'FP_1368', 'FP_1372', 'FP_1375', 'FP_1377', 'FP_1378', 'FP_1383', 'FP_1388', 'FP_1389', 'FP_1395', 'FP_1404', 'FP_1407', 'FP_1409', 'FP_1425', 'FP_1426', 'FP_1428', 'FP_1432', 'FP_1435', 'FP_1437', 'FP_1441', 'FP_1443', 'FP_1448', 'FP_1450', 'FP_1455', 'FP_1459', 'FP_1466', 'FP_1467', 'FP_1468', 'FP_1469', 'FP_1471', 'FP_1475', 'FP_1483', 'FP_1484', 'FP_1491', 'FP_1493', 'FP_1494', 'FP_1496', 'FP_1512', 'FP_1515', 'FP_1517', 'FP_1525', 'FP_1528', 'FP_1529', 'FP_1530', 'FP_1531', 'FP_1537', 'FP_1538', 'FP_1541', 'FP_1545', 'FP_1547', 'FP_1551', 'FP_1553', 'FP_1554', 'FP_1558', 'FP_1562', 'FP_1566', 'FP_1569', 'FP_1572', 'FP_1576', 'FP_1585', 'FP_1586', 'FP_1594', 'FP_1595', 'FP_1600', 'FP_1605', 'FP_1606', 'FP_1614', 'FP_1616', 'FP_1621', 'FP_1622', 'FP_1628', 'FP_1635', 'FP_1637', 'FP_1639', 'FP_1642', 'FP_1651', 'FP_1662', 'FP_1674', 'FP_1675', 'FP_1679', 'FP_1681', 'FP_1682', 'FP_1694', 'FP_1699', 'FP_1701', 'FP_1702', 'FP_1703', 'FP_1705', 'FP_1708', 'FP_1712', 'FP_1732', 'FP_1736', 'FP_1739', 'FP_1740', 'FP_1743', 'FP_1748', 'FP_1757', 'FP_1759', 'FP_1762', 'FP_1767', 'FP_1769', 'FP_1772', 'FP_1776', 'FP_1778', 'FP_1798', 'FP_1800', 'FP_1801', 'FP_1804', 'FP_1805', 'FP_1813', 'FP_1815', 'FP_1817', 'FP_1819', 'FP_1829', 'FP_1833', 'FP_1836', 'FP_1838', 'FP_1841', 'FP_1852', 'FP_1868', 'FP_1869', 'FP_1872', 'FP_1874', 'FP_1888', 'FP_1889', 'FP_1890', 'FP_1893', 'FP_1894', 'FP_1896', 'FP_1897', 'FP_1898', 'FP_1900', 'FP_1901', 'FP_1903', 'FP_1908', 'FP_1913', 'FP_1919', 'FP_1927', 'FP_1930', 'FP_1931', 'FP_1933', 'FP_1937', 'FP_1938', 'FP_1944', 'FP_1945', 'FP_1950', 'FP_1955', 'FP_1958', 'FP_1965', 'FP_1968', 'FP_1979', 'FP_2011', 'FP_2016', 'FP_2025', 'FP_2029', 'FP_2035', 'FP_2036', 'FP_2038', 'FP_2046']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 191): ['FP_276', 'FP_277', 'FP_331', 'FP_359', 'FP_370', 'FP_385', 'FP_421', 'FP_423', 'FP_424', 'FP_432', 'FP_437', 'FP_449', 'FP_510', 'FP_511', 'FP_523', 'FP_529', 'FP_532', 'FP_536', 'FP_539', 'FP_542', 'FP_544', 'FP_574', 'FP_577', 'FP_598', 'FP_605', 'FP_614', 'FP_622', 'FP_668', 'FP_683', 'FP_685', 'FP_724', 'FP_730', 'FP_773', 'FP_786', 'FP_815', 'FP_816', 'FP_823', 'FP_853', 'FP_854', 'FP_855', 'FP_856', 'FP_874', 'FP_897', 'FP_900', 'FP_938', 'FP_941', 'FP_962', 'FP_968', 'FP_969', 'FP_970', 'FP_982', 'FP_985', 'FP_993', 'FP_999', 'FP_1002', 'FP_1009', 'FP_1041', 'FP_1050', 'FP_1070', 'FP_1073', 'FP_1078', 'FP_1085', 'FP_1091', 'FP_1108', 'FP_1110', 'FP_1115', 'FP_1124', 'FP_1127', 'FP_1131', 'FP_1135', 'FP_1140', 'FP_1146', 'FP_1158', 'FP_1159', 'FP_1165', 'FP_1178', 'FP_1189', 'FP_1191', 'FP_1198', 'FP_1206', 'FP_1207', 'FP_1222', 'FP_1228', 'FP_1230', 'FP_1241', 'FP_1246', 'FP_1265', 'FP_1272', 'FP_1288', 'FP_1301', 'FP_1302', 'FP_1330', 'FP_1342', 'FP_1346', 'FP_1348', 'FP_1352', 'FP_1354', 'FP_1363', 'FP_1374', 'FP_1381', 'FP_1390', 'FP_1392', 'FP_1396', 'FP_1397', 'FP_1401', 'FP_1412', 'FP_1416', 'FP_1419', 'FP_1424', 'FP_1433', 'FP_1434', 'FP_1439', 'FP_1456', 'FP_1462', 'FP_1473', 'FP_1481', 'FP_1489', 'FP_1505', 'FP_1511', 'FP_1540', 'FP_1548', 'FP_1560', 'FP_1568', 'FP_1570', 'FP_1579', 'FP_1582', 'FP_1590', 'FP_1591', 'FP_1596', 'FP_1597', 'FP_1608', 'FP_1610', 'FP_1618', 'FP_1629', 'FP_1640', 'FP_1641', 'FP_1646', 'FP_1648', 'FP_1660', 'FP_1670', 'FP_1684', 'FP_1689', 'FP_1695', 'FP_1711', 'FP_1735', 'FP_1751', 'FP_1780', 'FP_1783', 'FP_1786', 'FP_1792', 'FP_1793', 'FP_1826', 'FP_1830', 'FP_1848', 'FP_1851', 'FP_1857', 'FP_1858', 'FP_1863', 'FP_1864', 'FP_1870', 'FP_1871', 'FP_1875', 'FP_1878', 'FP_1879', 'FP_1884', 'FP_1887', 'FP_1891', 'FP_1892', 'FP_1895', 'FP_1899', 'FP_1902', 'FP_1914', 'FP_1925', 'FP_1929', 'FP_1946', 'FP_1956', 'FP_1961', 'FP_1977', 'FP_1988', 'FP_1994', 'FP_2003', 'FP_2007', 'FP_2010', 'FP_2014', 'FP_2028', 'FP_2030', 'FP_2031', 'FP_2037', 'FP_2039', 'FP_2040', 'FP_2047']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 191 | ['FP_276', 'FP_277', 'FP_331', 'FP_359', 'FP_370', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 1390 | ['NumRotatableBonds', 'RingCount', 'FP_1', 'FP_2', 'FP_3', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :    2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 1388 | ['FP_1', 'FP_2', 'FP_3', 'FP_4', 'FP_5', ...]\n",
      "\t2.3s = Fit runtime\n",
      "\t1778 features in original data used to generate 1778 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.45 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 130.25s of the 130.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-66.7559\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.41s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 118.53s of the 118.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-70.4402\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.51s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 104.64s of the 104.63s of remaining time.\n",
      "\t-68.5756\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.67s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 101.62s of the 101.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-69.1381\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.37s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 63.49s of the 63.49s of remaining time.\n",
      "\t-67.5715\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 61.83s of the 61.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 61.29s of the 61.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\t-72.6394\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.71s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 47.29s of the 47.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\t-66.0103\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.32s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 38.58s of the 38.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 147. Best iteration is:\n",
      "\t[136]\tvalid_set's rmse: 84.6006\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 149. Best iteration is:\n",
      "\t[38]\tvalid_set's rmse: 92.4326\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 149. Best iteration is:\n",
      "\t[35]\tvalid_set's rmse: 68.0066\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 155. Best iteration is:\n",
      "\t[155]\tvalid_set's rmse: 74.1904\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 170. Best iteration is:\n",
      "\t[170]\tvalid_set's rmse: 74.5322\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 182. Best iteration is:\n",
      "\t[181]\tvalid_set's rmse: 78.4289\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 196. Best iteration is:\n",
      "\t[83]\tvalid_set's rmse: 62.5406\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 237. Best iteration is:\n",
      "\t[217]\tvalid_set's rmse: 73.0512\n",
      "\t-76.4789\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.81s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1.42s of the 1.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.0s)\n",
      "\tTime limit exceeded... Skipping CatBoost_r177_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 130.25s of the 0.77s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.529, 'ExtraTreesMSE_BAG_L1': 0.294, 'LightGBMXT_BAG_L1': 0.176}\n",
      "\t-64.0471\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 131.87s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 457.1 rows/s (64 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tg\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.9.23\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       16.39 GB / 31.11 GB (52.7%)\n",
      "Disk Space Avail:   15.67 GB / 1862.21 GB (0.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 225s of the 900s of remaining time (25%).\n",
      "\t\tContext path: \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\FFV\\ds_sub_fit\\sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Training for Target: FFV | ‚è≥ Time Limit: 900s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 225s\n",
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\FFV\\ds_sub_fit\\sub_fit_ho\"\n",
      "Train Data Rows:    6248\n",
      "Train Data Columns: 2438\n",
      "Label Column:       FFV\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16776.47 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.64 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2037 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 11): ['FP_187', 'FP_311', 'FP_515', 'FP_894', 'FP_928', 'FP_995', 'FP_1057', 'FP_1078', 'FP_1469', 'FP_1488', 'FP_1740']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 86): ['FP_22', 'FP_64', 'FP_86', 'FP_89', 'FP_132', 'FP_225', 'FP_269', 'FP_303', 'FP_318', 'FP_349', 'FP_356', 'FP_393', 'FP_436', 'FP_478', 'FP_509', 'FP_537', 'FP_572', 'FP_605', 'FP_614', 'FP_627', 'FP_630', 'FP_639', 'FP_688', 'FP_702', 'FP_719', 'FP_724', 'FP_735', 'FP_740', 'FP_776', 'FP_779', 'FP_784', 'FP_812', 'FP_822', 'FP_874', 'FP_907', 'FP_923', 'FP_925', 'FP_970', 'FP_982', 'FP_999', 'FP_1006', 'FP_1008', 'FP_1037', 'FP_1111', 'FP_1190', 'FP_1195', 'FP_1198', 'FP_1201', 'FP_1206', 'FP_1219', 'FP_1248', 'FP_1251', 'FP_1253', 'FP_1301', 'FP_1305', 'FP_1334', 'FP_1345', 'FP_1363', 'FP_1375', 'FP_1395', 'FP_1425', 'FP_1433', 'FP_1459', 'FP_1494', 'FP_1547', 'FP_1553', 'FP_1560', 'FP_1569', 'FP_1572', 'FP_1659', 'FP_1660', 'FP_1664', 'FP_1732', 'FP_1736', 'FP_1748', 'FP_1769', 'FP_1782', 'FP_1819', 'FP_1822', 'FP_1833', 'FP_1838', 'FP_1857', 'FP_1894', 'FP_1937', 'FP_2008', 'FP_2031']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 86 | ['FP_22', 'FP_64', 'FP_86', 'FP_89', 'FP_132', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 1953 | ['NumRotatableBonds', 'RingCount', 'FP_0', 'FP_1', 'FP_2', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :    2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 1951 | ['FP_0', 'FP_1', 'FP_2', 'FP_3', 'FP_4', ...]\n",
      "\t4.5s = Fit runtime\n",
      "\t2341 features in original data used to generate 2341 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.06 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 146.87s of the 220.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.00973463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1622. Best iteration is:\n",
      "\t[1622]\tvalid_set's rmse: 0.0096686\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0172238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1675. Best iteration is:\n",
      "\t[1632]\tvalid_set's rmse: 0.0171889\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0151593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1731. Best iteration is:\n",
      "\t[1719]\tvalid_set's rmse: 0.0151221\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.00971409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1792. Best iteration is:\n",
      "\t[1760]\tvalid_set's rmse: 0.00967862\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0106584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1946. Best iteration is:\n",
      "\t[1944]\tvalid_set's rmse: 0.0106059\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0101433\n",
      "[2000]\tvalid_set's rmse: 0.0100973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2094. Best iteration is:\n",
      "\t[2085]\tvalid_set's rmse: 0.0100942\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0145299\n",
      "[2000]\tvalid_set's rmse: 0.0144583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2320. Best iteration is:\n",
      "\t[2207]\tvalid_set's rmse: 0.0144505\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.00985083\n",
      "[2000]\tvalid_set's rmse: 0.00981746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2811. Best iteration is:\n",
      "\t[2529]\tvalid_set's rmse: 0.0098078\n",
      "\t-0.0124\t = Validation score   (-root_mean_squared_error)\n",
      "\t140.24s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5.67s of the 79.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's rmse: 0.0276517\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 4.19s of the 77.67s of remaining time.\n",
      "\t-0.015\t = Validation score   (-root_mean_squared_error)\n",
      "\t60.0s\t = Training   runtime\n",
      "\t1.95s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 220.36s of the 15.30s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.0124\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 15.29s of the 15.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 92. Best iteration is:\n",
      "\t[88]\tvalid_set's rmse: 0.0146973\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 95. Best iteration is:\n",
      "\t[95]\tvalid_set's rmse: 0.0105995\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 95. Best iteration is:\n",
      "\t[92]\tvalid_set's rmse: 0.00939422\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 103. Best iteration is:\n",
      "\t[94]\tvalid_set's rmse: 0.00931376\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 114. Best iteration is:\n",
      "\t[75]\tvalid_set's rmse: 0.0176547\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 125. Best iteration is:\n",
      "\t[97]\tvalid_set's rmse: 0.0151807\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 142. Best iteration is:\n",
      "\t[138]\tvalid_set's rmse: 0.0101734\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 181. Best iteration is:\n",
      "\t[174]\tvalid_set's rmse: 0.0113881\n",
      "\t-0.0126\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.18s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 220.36s of the 0.38s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.75, 'LightGBMXT_BAG_L2': 0.25}\n",
      "\t-0.0124\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 224.65s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1590.8 rows/s (781 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\FFV\\ds_sub_fit\\sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       LightGBMXT_BAG_L1      -0.020581  -0.012403  root_mean_squared_error        0.302024       0.146016  140.236885                 0.302024                0.146016         140.236885            1       True          1\n",
      "1     WeightedEnsemble_L2      -0.020581  -0.012403  root_mean_squared_error        0.309523       0.146016  140.240384                 0.007499                0.000000           0.003500            2       True          3\n",
      "2     WeightedEnsemble_L3      -0.020670  -0.012370  root_mean_squared_error        0.612750       2.201750  214.424195                 0.007506                0.000502           0.004501            3       True          5\n",
      "3       LightGBMXT_BAG_L2      -0.021120  -0.012643  root_mean_squared_error        0.605244       2.201248  214.419695                 0.169012                0.100514          14.181790            2       True          4\n",
      "4  RandomForestMSE_BAG_L1      -0.023347  -0.014953  root_mean_squared_error        0.134208       1.954718   60.001020                 0.134208                1.954718          60.001020            1       True          2\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t226s\t = DyStack   runtime |\t674s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 674s\n",
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\FFV\"\n",
      "Train Data Rows:    7030\n",
      "Train Data Columns: 2438\n",
      "Label Column:       FFV\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16956.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 24.35 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2041 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 7): ['FP_515', 'FP_894', 'FP_928', 'FP_1057', 'FP_1469', 'FP_1488', 'FP_1740']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 78): ['FP_30', 'FP_40', 'FP_44', 'FP_51', 'FP_64', 'FP_93', 'FP_159', 'FP_187', 'FP_303', 'FP_311', 'FP_318', 'FP_355', 'FP_356', 'FP_370', 'FP_408', 'FP_415', 'FP_418', 'FP_509', 'FP_516', 'FP_522', 'FP_553', 'FP_568', 'FP_614', 'FP_639', 'FP_651', 'FP_697', 'FP_700', 'FP_706', 'FP_719', 'FP_748', 'FP_776', 'FP_779', 'FP_812', 'FP_820', 'FP_830', 'FP_874', 'FP_912', 'FP_923', 'FP_982', 'FP_995', 'FP_1006', 'FP_1094', 'FP_1102', 'FP_1111', 'FP_1131', 'FP_1219', 'FP_1248', 'FP_1251', 'FP_1343', 'FP_1397', 'FP_1422', 'FP_1425', 'FP_1433', 'FP_1455', 'FP_1463', 'FP_1494', 'FP_1537', 'FP_1549', 'FP_1558', 'FP_1560', 'FP_1569', 'FP_1572', 'FP_1646', 'FP_1676', 'FP_1736', 'FP_1767', 'FP_1833', 'FP_1841', 'FP_1850', 'FP_1857', 'FP_1870', 'FP_1872', 'FP_1874', 'FP_1875', 'FP_1890', 'FP_1894', 'FP_1935', 'FP_1937']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 78 | ['FP_30', 'FP_40', 'FP_44', 'FP_51', 'FP_64', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 1965 | ['NumRotatableBonds', 'RingCount', 'FP_0', 'FP_1', 'FP_2', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :    2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 1963 | ['FP_0', 'FP_1', 'FP_2', 'FP_3', 'FP_4', ...]\n",
      "\t4.2s = Fit runtime\n",
      "\t2353 features in original data used to generate 2353 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 23.78 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 669.86s of the 669.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.00919546\n",
      "[2000]\tvalid_set's rmse: 0.00908408\n",
      "[3000]\tvalid_set's rmse: 0.00907\n",
      "[4000]\tvalid_set's rmse: 0.00906146\n",
      "[5000]\tvalid_set's rmse: 0.00906019\n",
      "[6000]\tvalid_set's rmse: 0.00905963\n",
      "[7000]\tvalid_set's rmse: 0.00905956\n",
      "[8000]\tvalid_set's rmse: 0.00905945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 8376. Best iteration is:\n",
      "\t[7846]\tvalid_set's rmse: 0.00905943\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0180526\n",
      "[2000]\tvalid_set's rmse: 0.0179781\n",
      "[3000]\tvalid_set's rmse: 0.017965\n",
      "[4000]\tvalid_set's rmse: 0.0179626\n",
      "[5000]\tvalid_set's rmse: 0.0179629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0107186\n",
      "[2000]\tvalid_set's rmse: 0.0106481\n",
      "[3000]\tvalid_set's rmse: 0.0106321\n",
      "[4000]\tvalid_set's rmse: 0.0106283\n",
      "[5000]\tvalid_set's rmse: 0.0106258\n",
      "[6000]\tvalid_set's rmse: 0.0106254\n",
      "[7000]\tvalid_set's rmse: 0.0106252\n",
      "[8000]\tvalid_set's rmse: 0.0106252\n",
      "[9000]\tvalid_set's rmse: 0.0106252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 9367. Best iteration is:\n",
      "\t[7761]\tvalid_set's rmse: 0.0106251\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0111692\n",
      "[2000]\tvalid_set's rmse: 0.0110814\n",
      "[3000]\tvalid_set's rmse: 0.0110719\n",
      "[4000]\tvalid_set's rmse: 0.0110677\n",
      "[5000]\tvalid_set's rmse: 0.0110661\n",
      "[6000]\tvalid_set's rmse: 0.0110658\n",
      "[7000]\tvalid_set's rmse: 0.0110658\n",
      "[8000]\tvalid_set's rmse: 0.0110657\n",
      "[9000]\tvalid_set's rmse: 0.0110657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 9707. Best iteration is:\n",
      "\t[9539]\tvalid_set's rmse: 0.0110657\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0175707\n",
      "[2000]\tvalid_set's rmse: 0.0174785\n",
      "[3000]\tvalid_set's rmse: 0.0174648\n",
      "[4000]\tvalid_set's rmse: 0.01746\n",
      "[5000]\tvalid_set's rmse: 0.0174587\n",
      "[6000]\tvalid_set's rmse: 0.0174583\n",
      "[7000]\tvalid_set's rmse: 0.0174582\n",
      "[8000]\tvalid_set's rmse: 0.0174581\n",
      "[9000]\tvalid_set's rmse: 0.0174581\n",
      "[10000]\tvalid_set's rmse: 0.0174581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0103131\n",
      "[2000]\tvalid_set's rmse: 0.0102459\n",
      "[3000]\tvalid_set's rmse: 0.0102368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0172698\n",
      "[2000]\tvalid_set's rmse: 0.0171689\n",
      "[3000]\tvalid_set's rmse: 0.0171576\n",
      "[4000]\tvalid_set's rmse: 0.0171515\n",
      "[5000]\tvalid_set's rmse: 0.0171515\n",
      "[6000]\tvalid_set's rmse: 0.0171506\n",
      "[7000]\tvalid_set's rmse: 0.0171504\n",
      "[8000]\tvalid_set's rmse: 0.0171504\n",
      "[9000]\tvalid_set's rmse: 0.0171503\n",
      "[10000]\tvalid_set's rmse: 0.0171503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0101842\n",
      "[2000]\tvalid_set's rmse: 0.0100984\n",
      "[3000]\tvalid_set's rmse: 0.0100841\n",
      "[4000]\tvalid_set's rmse: 0.0100841\n",
      "[5000]\tvalid_set's rmse: 0.0100826\n",
      "[6000]\tvalid_set's rmse: 0.0100821\n",
      "[7000]\tvalid_set's rmse: 0.010082\n",
      "[8000]\tvalid_set's rmse: 0.0100819\n",
      "[9000]\tvalid_set's rmse: 0.0100818\n",
      "[10000]\tvalid_set's rmse: 0.0100818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0134\t = Validation score   (-root_mean_squared_error)\n",
      "\t532.94s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 134.91s of the 134.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.00949193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1223. Best iteration is:\n",
      "\t[1220]\tvalid_set's rmse: 0.00945937\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0185569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1269. Best iteration is:\n",
      "\t[1258]\tvalid_set's rmse: 0.0185427\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0112728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1386. Best iteration is:\n",
      "\t[1196]\tvalid_set's rmse: 0.0112578\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0115078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0173556\n",
      "[2000]\tvalid_set's rmse: 0.0173122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2365. Best iteration is:\n",
      "\t[2330]\tvalid_set's rmse: 0.0173087\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0106117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0142\t = Validation score   (-root_mean_squared_error)\n",
      "\t115.32s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 18.70s of the 18.70s of remaining time.\n",
      "\t-0.0157\t = Validation score   (-root_mean_squared_error)\n",
      "\t70.34s\t = Training   runtime\n",
      "\t2.97s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -55.07s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.875, 'LightGBM_BAG_L1': 0.125}\n",
      "\t-0.0134\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 729.27s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1801.0 rows/s (879 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\FFV\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.9.23\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       16.37 GB / 31.11 GB (52.6%)\n",
      "Disk Space Avail:   15.37 GB / 1862.21 GB (0.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 60s of the 240s of remaining time (25%).\n",
      "\t\tContext path: \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tc\\ds_sub_fit\\sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 60s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Training for Target: Tc | ‚è≥ Time Limit: 240s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tc\\ds_sub_fit\\sub_fit_ho\"\n",
      "Train Data Rows:    655\n",
      "Train Data Columns: 2438\n",
      "Label Column:       Tc\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16742.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.27 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1257 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 791): ['FP_0', 'FP_3', 'FP_6', 'FP_10', 'FP_17', 'FP_18', 'FP_19', 'FP_20', 'FP_21', 'FP_22', 'FP_23', 'FP_27', 'FP_28', 'FP_30', 'FP_31', 'FP_35', 'FP_36', 'FP_40', 'FP_43', 'FP_44', 'FP_46', 'FP_48', 'FP_49', 'FP_51', 'FP_52', 'FP_53', 'FP_55', 'FP_63', 'FP_64', 'FP_66', 'FP_72', 'FP_73', 'FP_75', 'FP_77', 'FP_78', 'FP_82', 'FP_85', 'FP_86', 'FP_88', 'FP_91', 'FP_93', 'FP_96', 'FP_97', 'FP_98', 'FP_99', 'FP_101', 'FP_104', 'FP_106', 'FP_107', 'FP_110', 'FP_111', 'FP_115', 'FP_120', 'FP_122', 'FP_123', 'FP_124', 'FP_127', 'FP_131', 'FP_132', 'FP_134', 'FP_135', 'FP_139', 'FP_141', 'FP_143', 'FP_148', 'FP_150', 'FP_153', 'FP_155', 'FP_157', 'FP_159', 'FP_160', 'FP_166', 'FP_169', 'FP_171', 'FP_176', 'FP_177', 'FP_178', 'FP_181', 'FP_183', 'FP_187', 'FP_188', 'FP_192', 'FP_198', 'FP_207', 'FP_209', 'FP_211', 'FP_213', 'FP_214', 'FP_215', 'FP_220', 'FP_221', 'FP_225', 'FP_228', 'FP_229', 'FP_236', 'FP_239', 'FP_243', 'FP_247', 'FP_253', 'FP_256', 'FP_257', 'FP_258', 'FP_259', 'FP_260', 'FP_261', 'FP_267', 'FP_268', 'FP_269', 'FP_275', 'FP_277', 'FP_281', 'FP_284', 'FP_285', 'FP_286', 'FP_292', 'FP_293', 'FP_297', 'FP_298', 'FP_299', 'FP_300', 'FP_301', 'FP_302', 'FP_309', 'FP_312', 'FP_317', 'FP_318', 'FP_320', 'FP_321', 'FP_324', 'FP_326', 'FP_331', 'FP_332', 'FP_336', 'FP_337', 'FP_338', 'FP_343', 'FP_344', 'FP_346', 'FP_349', 'FP_355', 'FP_356', 'FP_358', 'FP_359', 'FP_362', 'FP_363', 'FP_368', 'FP_369', 'FP_370', 'FP_373', 'FP_376', 'FP_381', 'FP_384', 'FP_387', 'FP_388', 'FP_391', 'FP_393', 'FP_395', 'FP_397', 'FP_398', 'FP_399', 'FP_402', 'FP_405', 'FP_408', 'FP_415', 'FP_416', 'FP_417', 'FP_418', 'FP_423', 'FP_424', 'FP_426', 'FP_428', 'FP_431', 'FP_433', 'FP_434', 'FP_435', 'FP_436', 'FP_437', 'FP_439', 'FP_443', 'FP_449', 'FP_453', 'FP_454', 'FP_455', 'FP_457', 'FP_461', 'FP_462', 'FP_465', 'FP_466', 'FP_467', 'FP_470', 'FP_474', 'FP_475', 'FP_476', 'FP_477', 'FP_478', 'FP_480', 'FP_482', 'FP_483', 'FP_491', 'FP_494', 'FP_498', 'FP_505', 'FP_506', 'FP_509', 'FP_510', 'FP_511', 'FP_512', 'FP_513', 'FP_515', 'FP_516', 'FP_521', 'FP_524', 'FP_526', 'FP_527', 'FP_529', 'FP_531', 'FP_532', 'FP_533', 'FP_537', 'FP_542', 'FP_545', 'FP_547', 'FP_548', 'FP_549', 'FP_557', 'FP_558', 'FP_566', 'FP_568', 'FP_571', 'FP_572', 'FP_574', 'FP_575', 'FP_577', 'FP_578', 'FP_580', 'FP_583', 'FP_586', 'FP_593', 'FP_595', 'FP_596', 'FP_601', 'FP_605', 'FP_608', 'FP_611', 'FP_612', 'FP_613', 'FP_614', 'FP_618', 'FP_621', 'FP_623', 'FP_627', 'FP_630', 'FP_631', 'FP_633', 'FP_634', 'FP_635', 'FP_639', 'FP_642', 'FP_643', 'FP_646', 'FP_647', 'FP_648', 'FP_649', 'FP_651', 'FP_653', 'FP_655', 'FP_657', 'FP_658', 'FP_660', 'FP_661', 'FP_662', 'FP_669', 'FP_671', 'FP_676', 'FP_681', 'FP_683', 'FP_684', 'FP_688', 'FP_696', 'FP_697', 'FP_700', 'FP_704', 'FP_707', 'FP_708', 'FP_711', 'FP_713', 'FP_714', 'FP_723', 'FP_724', 'FP_726', 'FP_727', 'FP_733', 'FP_734', 'FP_735', 'FP_738', 'FP_740', 'FP_743', 'FP_744', 'FP_749', 'FP_751', 'FP_752', 'FP_756', 'FP_758', 'FP_761', 'FP_763', 'FP_767', 'FP_770', 'FP_771', 'FP_772', 'FP_773', 'FP_776', 'FP_779', 'FP_780', 'FP_782', 'FP_784', 'FP_788', 'FP_792', 'FP_793', 'FP_796', 'FP_797', 'FP_800', 'FP_804', 'FP_806', 'FP_808', 'FP_810', 'FP_811', 'FP_812', 'FP_818', 'FP_819', 'FP_823', 'FP_824', 'FP_830', 'FP_840', 'FP_846', 'FP_848', 'FP_851', 'FP_852', 'FP_853', 'FP_854', 'FP_855', 'FP_856', 'FP_858', 'FP_860', 'FP_861', 'FP_865', 'FP_871', 'FP_873', 'FP_874', 'FP_883', 'FP_884', 'FP_888', 'FP_889', 'FP_894', 'FP_903', 'FP_904', 'FP_905', 'FP_910', 'FP_911', 'FP_912', 'FP_918', 'FP_919', 'FP_920', 'FP_922', 'FP_923', 'FP_925', 'FP_928', 'FP_929', 'FP_939', 'FP_942', 'FP_944', 'FP_946', 'FP_947', 'FP_955', 'FP_957', 'FP_958', 'FP_962', 'FP_966', 'FP_968', 'FP_970', 'FP_971', 'FP_979', 'FP_981', 'FP_983', 'FP_988', 'FP_990', 'FP_991', 'FP_993', 'FP_995', 'FP_996', 'FP_999', 'FP_1002', 'FP_1006', 'FP_1007', 'FP_1009', 'FP_1010', 'FP_1015', 'FP_1016', 'FP_1018', 'FP_1021', 'FP_1023', 'FP_1026', 'FP_1033', 'FP_1036', 'FP_1040', 'FP_1042', 'FP_1044', 'FP_1048', 'FP_1054', 'FP_1057', 'FP_1067', 'FP_1068', 'FP_1069', 'FP_1073', 'FP_1074', 'FP_1078', 'FP_1079', 'FP_1080', 'FP_1082', 'FP_1092', 'FP_1094', 'FP_1102', 'FP_1105', 'FP_1106', 'FP_1107', 'FP_1111', 'FP_1117', 'FP_1122', 'FP_1124', 'FP_1131', 'FP_1132', 'FP_1134', 'FP_1137', 'FP_1144', 'FP_1146', 'FP_1149', 'FP_1150', 'FP_1151', 'FP_1158', 'FP_1165', 'FP_1167', 'FP_1168', 'FP_1170', 'FP_1177', 'FP_1179', 'FP_1181', 'FP_1183', 'FP_1186', 'FP_1188', 'FP_1192', 'FP_1193', 'FP_1194', 'FP_1200', 'FP_1201', 'FP_1202', 'FP_1204', 'FP_1205', 'FP_1207', 'FP_1208', 'FP_1214', 'FP_1215', 'FP_1216', 'FP_1217', 'FP_1219', 'FP_1223', 'FP_1226', 'FP_1231', 'FP_1233', 'FP_1234', 'FP_1242', 'FP_1245', 'FP_1246', 'FP_1247', 'FP_1248', 'FP_1250', 'FP_1253', 'FP_1254', 'FP_1255', 'FP_1262', 'FP_1266', 'FP_1269', 'FP_1275', 'FP_1277', 'FP_1281', 'FP_1289', 'FP_1291', 'FP_1293', 'FP_1296', 'FP_1297', 'FP_1299', 'FP_1300', 'FP_1301', 'FP_1305', 'FP_1306', 'FP_1307', 'FP_1311', 'FP_1314', 'FP_1315', 'FP_1316', 'FP_1318', 'FP_1319', 'FP_1320', 'FP_1322', 'FP_1329', 'FP_1330', 'FP_1334', 'FP_1336', 'FP_1341', 'FP_1342', 'FP_1343', 'FP_1345', 'FP_1348', 'FP_1355', 'FP_1358', 'FP_1361', 'FP_1364', 'FP_1367', 'FP_1372', 'FP_1373', 'FP_1374', 'FP_1375', 'FP_1376', 'FP_1387', 'FP_1394', 'FP_1395', 'FP_1396', 'FP_1397', 'FP_1400', 'FP_1401', 'FP_1404', 'FP_1406', 'FP_1407', 'FP_1409', 'FP_1412', 'FP_1414', 'FP_1418', 'FP_1422', 'FP_1424', 'FP_1425', 'FP_1426', 'FP_1433', 'FP_1434', 'FP_1435', 'FP_1439', 'FP_1442', 'FP_1443', 'FP_1445', 'FP_1449', 'FP_1457', 'FP_1458', 'FP_1459', 'FP_1460', 'FP_1463', 'FP_1466', 'FP_1467', 'FP_1469', 'FP_1471', 'FP_1475', 'FP_1477', 'FP_1481', 'FP_1483', 'FP_1484', 'FP_1486', 'FP_1488', 'FP_1492', 'FP_1493', 'FP_1494', 'FP_1495', 'FP_1496', 'FP_1499', 'FP_1501', 'FP_1502', 'FP_1504', 'FP_1505', 'FP_1512', 'FP_1513', 'FP_1514', 'FP_1515', 'FP_1517', 'FP_1521', 'FP_1523', 'FP_1525', 'FP_1528', 'FP_1529', 'FP_1532', 'FP_1537', 'FP_1538', 'FP_1539', 'FP_1542', 'FP_1546', 'FP_1547', 'FP_1548', 'FP_1549', 'FP_1551', 'FP_1552', 'FP_1553', 'FP_1558', 'FP_1560', 'FP_1562', 'FP_1563', 'FP_1568', 'FP_1569', 'FP_1570', 'FP_1576', 'FP_1577', 'FP_1579', 'FP_1580', 'FP_1584', 'FP_1585', 'FP_1586', 'FP_1587', 'FP_1590', 'FP_1593', 'FP_1595', 'FP_1596', 'FP_1600', 'FP_1605', 'FP_1606', 'FP_1608', 'FP_1613', 'FP_1614', 'FP_1615', 'FP_1616', 'FP_1619', 'FP_1622', 'FP_1623', 'FP_1625', 'FP_1627', 'FP_1629', 'FP_1630', 'FP_1632', 'FP_1633', 'FP_1635', 'FP_1636', 'FP_1637', 'FP_1639', 'FP_1641', 'FP_1651', 'FP_1657', 'FP_1660', 'FP_1663', 'FP_1664', 'FP_1667', 'FP_1672', 'FP_1674', 'FP_1676', 'FP_1677', 'FP_1678', 'FP_1679', 'FP_1681', 'FP_1689', 'FP_1690', 'FP_1694', 'FP_1696', 'FP_1701', 'FP_1702', 'FP_1703', 'FP_1704', 'FP_1705', 'FP_1709', 'FP_1711', 'FP_1727', 'FP_1731', 'FP_1736', 'FP_1738', 'FP_1740', 'FP_1744', 'FP_1746', 'FP_1748', 'FP_1749', 'FP_1753', 'FP_1756', 'FP_1759', 'FP_1762', 'FP_1764', 'FP_1765', 'FP_1766', 'FP_1767', 'FP_1768', 'FP_1769', 'FP_1776', 'FP_1779', 'FP_1780', 'FP_1781', 'FP_1782', 'FP_1786', 'FP_1792', 'FP_1793', 'FP_1796', 'FP_1797', 'FP_1800', 'FP_1802', 'FP_1803', 'FP_1805', 'FP_1807', 'FP_1808', 'FP_1812', 'FP_1813', 'FP_1815', 'FP_1817', 'FP_1819', 'FP_1821', 'FP_1822', 'FP_1828', 'FP_1829', 'FP_1830', 'FP_1833', 'FP_1835', 'FP_1837', 'FP_1838', 'FP_1841', 'FP_1843', 'FP_1844', 'FP_1848', 'FP_1849', 'FP_1851', 'FP_1852', 'FP_1857', 'FP_1858', 'FP_1864', 'FP_1865', 'FP_1869', 'FP_1870', 'FP_1874', 'FP_1875', 'FP_1877', 'FP_1881', 'FP_1882', 'FP_1884', 'FP_1888', 'FP_1889', 'FP_1890', 'FP_1892', 'FP_1894', 'FP_1895', 'FP_1896', 'FP_1897', 'FP_1902', 'FP_1903', 'FP_1906', 'FP_1907', 'FP_1910', 'FP_1912', 'FP_1922', 'FP_1925', 'FP_1929', 'FP_1931', 'FP_1932', 'FP_1933', 'FP_1935', 'FP_1937', 'FP_1939', 'FP_1941', 'FP_1944', 'FP_1945', 'FP_1946', 'FP_1958', 'FP_1959', 'FP_1960', 'FP_1962', 'FP_1965', 'FP_1967', 'FP_1969', 'FP_1971', 'FP_1983', 'FP_1984', 'FP_1988', 'FP_1994', 'FP_2000', 'FP_2001', 'FP_2003', 'FP_2005', 'FP_2007', 'FP_2008', 'FP_2010', 'FP_2011', 'FP_2012', 'FP_2014', 'FP_2015', 'FP_2016', 'FP_2019', 'FP_2021', 'FP_2022', 'FP_2026', 'FP_2028', 'FP_2029', 'FP_2031', 'FP_2035', 'FP_2036', 'FP_2037', 'FP_2038', 'FP_2039']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 256): ['FP_144', 'FP_158', 'FP_186', 'FP_206', 'FP_217', 'FP_238', 'FP_414', 'FP_430', 'FP_432', 'FP_451', 'FP_458', 'FP_459', 'FP_464', 'FP_468', 'FP_485', 'FP_492', 'FP_496', 'FP_499', 'FP_507', 'FP_519', 'FP_525', 'FP_528', 'FP_534', 'FP_544', 'FP_550', 'FP_551', 'FP_554', 'FP_559', 'FP_564', 'FP_569', 'FP_594', 'FP_602', 'FP_609', 'FP_644', 'FP_665', 'FP_672', 'FP_673', 'FP_678', 'FP_687', 'FP_702', 'FP_709', 'FP_722', 'FP_737', 'FP_742', 'FP_750', 'FP_757', 'FP_760', 'FP_764', 'FP_786', 'FP_826', 'FP_839', 'FP_842', 'FP_862', 'FP_863', 'FP_867', 'FP_868', 'FP_876', 'FP_882', 'FP_885', 'FP_887', 'FP_909', 'FP_924', 'FP_927', 'FP_949', 'FP_951', 'FP_954', 'FP_956', 'FP_977', 'FP_980', 'FP_982', 'FP_989', 'FP_998', 'FP_1000', 'FP_1001', 'FP_1014', 'FP_1025', 'FP_1029', 'FP_1046', 'FP_1050', 'FP_1051', 'FP_1062', 'FP_1085', 'FP_1095', 'FP_1098', 'FP_1115', 'FP_1118', 'FP_1121', 'FP_1123', 'FP_1129', 'FP_1130', 'FP_1135', 'FP_1138', 'FP_1154', 'FP_1155', 'FP_1156', 'FP_1157', 'FP_1159', 'FP_1164', 'FP_1178', 'FP_1189', 'FP_1191', 'FP_1197', 'FP_1198', 'FP_1203', 'FP_1206', 'FP_1210', 'FP_1213', 'FP_1227', 'FP_1228', 'FP_1230', 'FP_1236', 'FP_1241', 'FP_1244', 'FP_1265', 'FP_1270', 'FP_1276', 'FP_1278', 'FP_1279', 'FP_1285', 'FP_1286', 'FP_1288', 'FP_1290', 'FP_1294', 'FP_1304', 'FP_1310', 'FP_1321', 'FP_1324', 'FP_1327', 'FP_1340', 'FP_1344', 'FP_1360', 'FP_1362', 'FP_1363', 'FP_1378', 'FP_1379', 'FP_1389', 'FP_1393', 'FP_1408', 'FP_1416', 'FP_1419', 'FP_1427', 'FP_1428', 'FP_1431', 'FP_1437', 'FP_1438', 'FP_1448', 'FP_1455', 'FP_1461', 'FP_1464', 'FP_1468', 'FP_1470', 'FP_1472', 'FP_1473', 'FP_1474', 'FP_1478', 'FP_1491', 'FP_1497', 'FP_1510', 'FP_1511', 'FP_1519', 'FP_1520', 'FP_1524', 'FP_1526', 'FP_1530', 'FP_1531', 'FP_1540', 'FP_1541', 'FP_1543', 'FP_1554', 'FP_1555', 'FP_1557', 'FP_1565', 'FP_1567', 'FP_1571', 'FP_1572', 'FP_1578', 'FP_1599', 'FP_1603', 'FP_1610', 'FP_1611', 'FP_1620', 'FP_1621', 'FP_1640', 'FP_1642', 'FP_1643', 'FP_1644', 'FP_1648', 'FP_1654', 'FP_1659', 'FP_1662', 'FP_1666', 'FP_1675', 'FP_1680', 'FP_1682', 'FP_1687', 'FP_1700', 'FP_1708', 'FP_1732', 'FP_1735', 'FP_1737', 'FP_1739', 'FP_1741', 'FP_1743', 'FP_1745', 'FP_1757', 'FP_1771', 'FP_1783', 'FP_1787', 'FP_1788', 'FP_1789', 'FP_1791', 'FP_1801', 'FP_1806', 'FP_1811', 'FP_1831', 'FP_1836', 'FP_1842', 'FP_1846', 'FP_1850', 'FP_1854', 'FP_1861', 'FP_1862', 'FP_1867', 'FP_1871', 'FP_1872', 'FP_1880', 'FP_1886', 'FP_1887', 'FP_1893', 'FP_1898', 'FP_1900', 'FP_1905', 'FP_1913', 'FP_1921', 'FP_1923', 'FP_1924', 'FP_1938', 'FP_1940', 'FP_1949', 'FP_1954', 'FP_1963', 'FP_1973', 'FP_1974', 'FP_1976', 'FP_1977', 'FP_1980', 'FP_1986', 'FP_1993', 'FP_2025', 'FP_2030', 'FP_2034', 'FP_2040', 'FP_2041', 'FP_2045', 'FP_2046', 'FP_2047']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 256 | ['FP_144', 'FP_158', 'FP_186', 'FP_206', 'FP_217', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 1003 | ['NumRotatableBonds', 'RingCount', 'FP_1', 'FP_2', 'FP_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :    2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 1001 | ['FP_1', 'FP_2', 'FP_4', 'FP_5', 'FP_7', ...]\n",
      "\t2.0s = Fit runtime\n",
      "\t1391 features in original data used to generate 1391 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.61 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 38.59s of the 57.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0379574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1453. Best iteration is:\n",
      "\t[1365]\tvalid_set's rmse: 0.0379257\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0386\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.01s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 20.33s of the 39.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 447. Best iteration is:\n",
      "\t[433]\tvalid_set's rmse: 0.0399735\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 518. Best iteration is:\n",
      "\t[518]\tvalid_set's rmse: 0.0567178\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0394\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.84s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3.20s of the 22.51s of remaining time.\n",
      "\t-0.0403\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.21s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 57.90s of the 18.94s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.812, 'RandomForestMSE_BAG_L1': 0.188}\n",
      "\t-0.0385\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 18.93s of the 18.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 522. Best iteration is:\n",
      "\t[517]\tvalid_set's rmse: 0.0431704\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0389\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.34s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 4.36s of the 4.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 9. Best iteration is:\n",
      "\t[9]\tvalid_set's rmse: 0.0732986\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 6. Best iteration is:\n",
      "\t[6]\tvalid_set's rmse: 0.0757588\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 7. Best iteration is:\n",
      "\t[7]\tvalid_set's rmse: 0.062053\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 14. Best iteration is:\n",
      "\t[14]\tvalid_set's rmse: 0.0603461\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\t[17]\tvalid_set's rmse: 0.0457275\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 23. Best iteration is:\n",
      "\t[23]\tvalid_set's rmse: 0.0404234\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 19. Best iteration is:\n",
      "\t[19]\tvalid_set's rmse: 0.0514057\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 45. Best iteration is:\n",
      "\t[43]\tvalid_set's rmse: 0.0348576\n",
      "\t-0.0572\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.0s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 57.90s of the 0.00s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.812, 'RandomForestMSE_BAG_L1': 0.188}\n",
      "\t-0.0385\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 60.01s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1526.3 rows/s (82 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tc\\ds_sub_fit\\sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  RandomForestMSE_BAG_L1      -0.052699  -0.040256  root_mean_squared_error        0.067007       0.133511   3.207801                 0.067007                0.133511           3.207801            1       True          3\n",
      "1         LightGBM_BAG_L1      -0.053515  -0.039419  root_mean_squared_error        0.099008       0.038003  16.843497                 0.099008                0.038003          16.843497            1       True          2\n",
      "2     WeightedEnsemble_L2      -0.053554  -0.038518  root_mean_squared_error        0.175012       0.170520  21.222830                 0.008500                0.000000           0.003000            2       True          4\n",
      "3     WeightedEnsemble_L3      -0.053554  -0.038518  root_mean_squared_error        0.175012       0.170520  21.223330                 0.008500                0.000000           0.003500            3       True          7\n",
      "4       LightGBMXT_BAG_L2      -0.053610  -0.038892  root_mean_squared_error        0.263520       0.212025  35.557075                 0.097008                0.041505          14.337245            2       True          5\n",
      "5       LightGBMXT_BAG_L1      -0.053994  -0.038614  root_mean_squared_error        0.099505       0.037009  18.012029                 0.099505                0.037009          18.012029            1       True          1\n",
      "6         LightGBM_BAG_L2      -0.063155  -0.057241  root_mean_squared_error        0.253023       0.213526  25.219199                 0.086511                0.043006           3.999368            2       True          6\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t61s\t = DyStack   runtime |\t179s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 179s\n",
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tc\"\n",
      "Train Data Rows:    737\n",
      "Train Data Columns: 2438\n",
      "Label Column:       Tc\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16951.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.55 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1299 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 749): ['FP_0', 'FP_3', 'FP_10', 'FP_17', 'FP_18', 'FP_19', 'FP_20', 'FP_21', 'FP_22', 'FP_23', 'FP_27', 'FP_28', 'FP_30', 'FP_31', 'FP_35', 'FP_36', 'FP_40', 'FP_43', 'FP_44', 'FP_46', 'FP_48', 'FP_49', 'FP_51', 'FP_52', 'FP_53', 'FP_55', 'FP_63', 'FP_64', 'FP_66', 'FP_72', 'FP_73', 'FP_75', 'FP_77', 'FP_78', 'FP_82', 'FP_86', 'FP_88', 'FP_91', 'FP_93', 'FP_96', 'FP_97', 'FP_98', 'FP_101', 'FP_106', 'FP_110', 'FP_111', 'FP_115', 'FP_120', 'FP_122', 'FP_123', 'FP_124', 'FP_127', 'FP_131', 'FP_134', 'FP_139', 'FP_141', 'FP_143', 'FP_148', 'FP_150', 'FP_153', 'FP_155', 'FP_157', 'FP_159', 'FP_160', 'FP_166', 'FP_169', 'FP_171', 'FP_176', 'FP_177', 'FP_178', 'FP_181', 'FP_183', 'FP_187', 'FP_188', 'FP_192', 'FP_198', 'FP_207', 'FP_209', 'FP_211', 'FP_213', 'FP_214', 'FP_215', 'FP_220', 'FP_221', 'FP_225', 'FP_228', 'FP_229', 'FP_236', 'FP_239', 'FP_243', 'FP_247', 'FP_253', 'FP_256', 'FP_257', 'FP_258', 'FP_259', 'FP_260', 'FP_261', 'FP_267', 'FP_268', 'FP_269', 'FP_275', 'FP_277', 'FP_281', 'FP_284', 'FP_285', 'FP_286', 'FP_292', 'FP_293', 'FP_297', 'FP_299', 'FP_300', 'FP_301', 'FP_302', 'FP_309', 'FP_312', 'FP_317', 'FP_318', 'FP_320', 'FP_321', 'FP_324', 'FP_326', 'FP_331', 'FP_332', 'FP_336', 'FP_337', 'FP_338', 'FP_343', 'FP_344', 'FP_346', 'FP_349', 'FP_355', 'FP_356', 'FP_358', 'FP_359', 'FP_363', 'FP_368', 'FP_369', 'FP_370', 'FP_373', 'FP_376', 'FP_381', 'FP_384', 'FP_387', 'FP_388', 'FP_391', 'FP_393', 'FP_395', 'FP_397', 'FP_398', 'FP_399', 'FP_402', 'FP_405', 'FP_408', 'FP_415', 'FP_416', 'FP_417', 'FP_418', 'FP_423', 'FP_424', 'FP_426', 'FP_428', 'FP_431', 'FP_433', 'FP_434', 'FP_435', 'FP_436', 'FP_437', 'FP_439', 'FP_443', 'FP_449', 'FP_453', 'FP_454', 'FP_455', 'FP_457', 'FP_462', 'FP_465', 'FP_466', 'FP_467', 'FP_470', 'FP_475', 'FP_476', 'FP_477', 'FP_478', 'FP_480', 'FP_482', 'FP_483', 'FP_491', 'FP_494', 'FP_498', 'FP_505', 'FP_506', 'FP_509', 'FP_510', 'FP_511', 'FP_512', 'FP_513', 'FP_515', 'FP_516', 'FP_521', 'FP_524', 'FP_526', 'FP_527', 'FP_529', 'FP_531', 'FP_532', 'FP_533', 'FP_537', 'FP_542', 'FP_545', 'FP_547', 'FP_548', 'FP_549', 'FP_557', 'FP_558', 'FP_566', 'FP_568', 'FP_571', 'FP_572', 'FP_574', 'FP_577', 'FP_578', 'FP_580', 'FP_583', 'FP_586', 'FP_593', 'FP_595', 'FP_596', 'FP_601', 'FP_605', 'FP_608', 'FP_611', 'FP_612', 'FP_613', 'FP_614', 'FP_618', 'FP_621', 'FP_623', 'FP_627', 'FP_630', 'FP_631', 'FP_633', 'FP_634', 'FP_635', 'FP_639', 'FP_642', 'FP_643', 'FP_646', 'FP_647', 'FP_648', 'FP_649', 'FP_651', 'FP_653', 'FP_657', 'FP_658', 'FP_660', 'FP_661', 'FP_662', 'FP_669', 'FP_671', 'FP_676', 'FP_681', 'FP_683', 'FP_684', 'FP_688', 'FP_696', 'FP_697', 'FP_700', 'FP_704', 'FP_707', 'FP_708', 'FP_711', 'FP_713', 'FP_714', 'FP_723', 'FP_724', 'FP_726', 'FP_727', 'FP_733', 'FP_734', 'FP_735', 'FP_738', 'FP_740', 'FP_743', 'FP_744', 'FP_751', 'FP_752', 'FP_756', 'FP_758', 'FP_761', 'FP_763', 'FP_767', 'FP_770', 'FP_771', 'FP_772', 'FP_773', 'FP_776', 'FP_779', 'FP_780', 'FP_782', 'FP_784', 'FP_788', 'FP_792', 'FP_793', 'FP_796', 'FP_797', 'FP_800', 'FP_804', 'FP_806', 'FP_808', 'FP_810', 'FP_811', 'FP_812', 'FP_818', 'FP_819', 'FP_823', 'FP_824', 'FP_830', 'FP_840', 'FP_846', 'FP_848', 'FP_851', 'FP_852', 'FP_853', 'FP_854', 'FP_855', 'FP_856', 'FP_858', 'FP_860', 'FP_861', 'FP_865', 'FP_871', 'FP_873', 'FP_874', 'FP_883', 'FP_884', 'FP_888', 'FP_889', 'FP_894', 'FP_903', 'FP_904', 'FP_910', 'FP_911', 'FP_912', 'FP_918', 'FP_919', 'FP_920', 'FP_922', 'FP_923', 'FP_925', 'FP_928', 'FP_939', 'FP_942', 'FP_944', 'FP_946', 'FP_947', 'FP_955', 'FP_957', 'FP_958', 'FP_966', 'FP_968', 'FP_970', 'FP_971', 'FP_979', 'FP_983', 'FP_988', 'FP_990', 'FP_991', 'FP_993', 'FP_995', 'FP_996', 'FP_999', 'FP_1002', 'FP_1006', 'FP_1007', 'FP_1009', 'FP_1010', 'FP_1015', 'FP_1016', 'FP_1018', 'FP_1021', 'FP_1023', 'FP_1026', 'FP_1033', 'FP_1040', 'FP_1042', 'FP_1044', 'FP_1048', 'FP_1054', 'FP_1057', 'FP_1067', 'FP_1068', 'FP_1073', 'FP_1074', 'FP_1078', 'FP_1079', 'FP_1080', 'FP_1082', 'FP_1092', 'FP_1094', 'FP_1102', 'FP_1105', 'FP_1106', 'FP_1107', 'FP_1111', 'FP_1117', 'FP_1122', 'FP_1124', 'FP_1131', 'FP_1132', 'FP_1134', 'FP_1137', 'FP_1144', 'FP_1146', 'FP_1149', 'FP_1150', 'FP_1151', 'FP_1158', 'FP_1165', 'FP_1167', 'FP_1168', 'FP_1170', 'FP_1177', 'FP_1179', 'FP_1181', 'FP_1183', 'FP_1186', 'FP_1188', 'FP_1192', 'FP_1193', 'FP_1194', 'FP_1200', 'FP_1201', 'FP_1202', 'FP_1204', 'FP_1205', 'FP_1207', 'FP_1208', 'FP_1214', 'FP_1215', 'FP_1216', 'FP_1217', 'FP_1219', 'FP_1223', 'FP_1231', 'FP_1234', 'FP_1242', 'FP_1245', 'FP_1246', 'FP_1247', 'FP_1248', 'FP_1250', 'FP_1253', 'FP_1254', 'FP_1255', 'FP_1262', 'FP_1266', 'FP_1275', 'FP_1277', 'FP_1281', 'FP_1289', 'FP_1291', 'FP_1293', 'FP_1296', 'FP_1299', 'FP_1300', 'FP_1301', 'FP_1305', 'FP_1306', 'FP_1307', 'FP_1311', 'FP_1314', 'FP_1315', 'FP_1316', 'FP_1318', 'FP_1319', 'FP_1320', 'FP_1322', 'FP_1329', 'FP_1330', 'FP_1334', 'FP_1336', 'FP_1341', 'FP_1342', 'FP_1343', 'FP_1345', 'FP_1348', 'FP_1358', 'FP_1361', 'FP_1364', 'FP_1367', 'FP_1372', 'FP_1373', 'FP_1374', 'FP_1375', 'FP_1376', 'FP_1387', 'FP_1394', 'FP_1395', 'FP_1396', 'FP_1397', 'FP_1400', 'FP_1401', 'FP_1406', 'FP_1407', 'FP_1409', 'FP_1412', 'FP_1418', 'FP_1422', 'FP_1424', 'FP_1425', 'FP_1426', 'FP_1433', 'FP_1434', 'FP_1435', 'FP_1439', 'FP_1442', 'FP_1445', 'FP_1449', 'FP_1457', 'FP_1458', 'FP_1459', 'FP_1460', 'FP_1463', 'FP_1467', 'FP_1469', 'FP_1471', 'FP_1475', 'FP_1477', 'FP_1481', 'FP_1483', 'FP_1484', 'FP_1488', 'FP_1492', 'FP_1493', 'FP_1494', 'FP_1495', 'FP_1496', 'FP_1499', 'FP_1501', 'FP_1502', 'FP_1504', 'FP_1505', 'FP_1512', 'FP_1513', 'FP_1514', 'FP_1515', 'FP_1517', 'FP_1521', 'FP_1523', 'FP_1525', 'FP_1528', 'FP_1529', 'FP_1532', 'FP_1537', 'FP_1538', 'FP_1546', 'FP_1547', 'FP_1548', 'FP_1549', 'FP_1552', 'FP_1553', 'FP_1558', 'FP_1560', 'FP_1562', 'FP_1563', 'FP_1568', 'FP_1569', 'FP_1570', 'FP_1576', 'FP_1577', 'FP_1580', 'FP_1584', 'FP_1585', 'FP_1586', 'FP_1587', 'FP_1593', 'FP_1595', 'FP_1596', 'FP_1600', 'FP_1605', 'FP_1606', 'FP_1608', 'FP_1613', 'FP_1614', 'FP_1615', 'FP_1616', 'FP_1619', 'FP_1622', 'FP_1623', 'FP_1625', 'FP_1627', 'FP_1629', 'FP_1630', 'FP_1632', 'FP_1633', 'FP_1635', 'FP_1636', 'FP_1637', 'FP_1639', 'FP_1641', 'FP_1651', 'FP_1657', 'FP_1660', 'FP_1663', 'FP_1664', 'FP_1667', 'FP_1672', 'FP_1674', 'FP_1676', 'FP_1677', 'FP_1678', 'FP_1679', 'FP_1689', 'FP_1690', 'FP_1694', 'FP_1696', 'FP_1701', 'FP_1702', 'FP_1703', 'FP_1704', 'FP_1705', 'FP_1709', 'FP_1711', 'FP_1727', 'FP_1731', 'FP_1736', 'FP_1738', 'FP_1740', 'FP_1744', 'FP_1746', 'FP_1748', 'FP_1749', 'FP_1753', 'FP_1756', 'FP_1759', 'FP_1762', 'FP_1765', 'FP_1766', 'FP_1767', 'FP_1768', 'FP_1769', 'FP_1776', 'FP_1779', 'FP_1780', 'FP_1781', 'FP_1782', 'FP_1786', 'FP_1792', 'FP_1793', 'FP_1796', 'FP_1797', 'FP_1800', 'FP_1802', 'FP_1803', 'FP_1805', 'FP_1807', 'FP_1808', 'FP_1812', 'FP_1813', 'FP_1815', 'FP_1817', 'FP_1819', 'FP_1821', 'FP_1822', 'FP_1828', 'FP_1829', 'FP_1830', 'FP_1833', 'FP_1835', 'FP_1837', 'FP_1838', 'FP_1841', 'FP_1843', 'FP_1844', 'FP_1848', 'FP_1849', 'FP_1851', 'FP_1852', 'FP_1857', 'FP_1858', 'FP_1864', 'FP_1865', 'FP_1870', 'FP_1874', 'FP_1875', 'FP_1877', 'FP_1881', 'FP_1882', 'FP_1884', 'FP_1889', 'FP_1890', 'FP_1892', 'FP_1894', 'FP_1895', 'FP_1896', 'FP_1897', 'FP_1902', 'FP_1903', 'FP_1906', 'FP_1907', 'FP_1910', 'FP_1922', 'FP_1925', 'FP_1929', 'FP_1931', 'FP_1932', 'FP_1933', 'FP_1935', 'FP_1937', 'FP_1939', 'FP_1941', 'FP_1944', 'FP_1945', 'FP_1946', 'FP_1958', 'FP_1959', 'FP_1960', 'FP_1962', 'FP_1965', 'FP_1967', 'FP_1969', 'FP_1971', 'FP_1983', 'FP_1988', 'FP_1994', 'FP_2000', 'FP_2001', 'FP_2003', 'FP_2005', 'FP_2008', 'FP_2010', 'FP_2011', 'FP_2012', 'FP_2014', 'FP_2015', 'FP_2016', 'FP_2019', 'FP_2021', 'FP_2022', 'FP_2026', 'FP_2028', 'FP_2029', 'FP_2031', 'FP_2035', 'FP_2036', 'FP_2037', 'FP_2038', 'FP_2039']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 250): ['FP_144', 'FP_186', 'FP_217', 'FP_238', 'FP_298', 'FP_414', 'FP_451', 'FP_458', 'FP_459', 'FP_461', 'FP_464', 'FP_468', 'FP_485', 'FP_492', 'FP_499', 'FP_507', 'FP_519', 'FP_525', 'FP_528', 'FP_544', 'FP_550', 'FP_551', 'FP_559', 'FP_564', 'FP_569', 'FP_594', 'FP_602', 'FP_609', 'FP_644', 'FP_665', 'FP_673', 'FP_678', 'FP_687', 'FP_702', 'FP_722', 'FP_737', 'FP_742', 'FP_750', 'FP_760', 'FP_764', 'FP_786', 'FP_826', 'FP_839', 'FP_842', 'FP_863', 'FP_868', 'FP_882', 'FP_885', 'FP_887', 'FP_905', 'FP_909', 'FP_927', 'FP_929', 'FP_949', 'FP_951', 'FP_954', 'FP_956', 'FP_962', 'FP_977', 'FP_980', 'FP_982', 'FP_989', 'FP_1000', 'FP_1001', 'FP_1014', 'FP_1025', 'FP_1046', 'FP_1050', 'FP_1051', 'FP_1062', 'FP_1085', 'FP_1095', 'FP_1098', 'FP_1115', 'FP_1118', 'FP_1121', 'FP_1123', 'FP_1129', 'FP_1130', 'FP_1135', 'FP_1138', 'FP_1154', 'FP_1155', 'FP_1156', 'FP_1157', 'FP_1159', 'FP_1178', 'FP_1189', 'FP_1191', 'FP_1197', 'FP_1198', 'FP_1203', 'FP_1206', 'FP_1210', 'FP_1226', 'FP_1227', 'FP_1228', 'FP_1230', 'FP_1233', 'FP_1236', 'FP_1241', 'FP_1244', 'FP_1265', 'FP_1270', 'FP_1276', 'FP_1278', 'FP_1279', 'FP_1285', 'FP_1288', 'FP_1294', 'FP_1304', 'FP_1310', 'FP_1321', 'FP_1324', 'FP_1327', 'FP_1340', 'FP_1344', 'FP_1360', 'FP_1362', 'FP_1378', 'FP_1379', 'FP_1389', 'FP_1393', 'FP_1408', 'FP_1416', 'FP_1419', 'FP_1427', 'FP_1428', 'FP_1431', 'FP_1437', 'FP_1438', 'FP_1443', 'FP_1448', 'FP_1455', 'FP_1461', 'FP_1464', 'FP_1466', 'FP_1468', 'FP_1470', 'FP_1472', 'FP_1473', 'FP_1474', 'FP_1478', 'FP_1486', 'FP_1491', 'FP_1497', 'FP_1510', 'FP_1511', 'FP_1519', 'FP_1520', 'FP_1524', 'FP_1526', 'FP_1530', 'FP_1531', 'FP_1539', 'FP_1540', 'FP_1541', 'FP_1543', 'FP_1554', 'FP_1555', 'FP_1557', 'FP_1565', 'FP_1567', 'FP_1571', 'FP_1572', 'FP_1578', 'FP_1579', 'FP_1590', 'FP_1599', 'FP_1603', 'FP_1610', 'FP_1611', 'FP_1620', 'FP_1621', 'FP_1640', 'FP_1642', 'FP_1643', 'FP_1644', 'FP_1648', 'FP_1654', 'FP_1659', 'FP_1662', 'FP_1666', 'FP_1675', 'FP_1680', 'FP_1681', 'FP_1682', 'FP_1687', 'FP_1700', 'FP_1708', 'FP_1732', 'FP_1735', 'FP_1737', 'FP_1739', 'FP_1741', 'FP_1743', 'FP_1757', 'FP_1764', 'FP_1771', 'FP_1783', 'FP_1787', 'FP_1788', 'FP_1789', 'FP_1791', 'FP_1801', 'FP_1806', 'FP_1811', 'FP_1831', 'FP_1836', 'FP_1842', 'FP_1846', 'FP_1850', 'FP_1861', 'FP_1862', 'FP_1867', 'FP_1869', 'FP_1871', 'FP_1872', 'FP_1880', 'FP_1886', 'FP_1887', 'FP_1888', 'FP_1898', 'FP_1900', 'FP_1905', 'FP_1912', 'FP_1913', 'FP_1921', 'FP_1923', 'FP_1924', 'FP_1938', 'FP_1940', 'FP_1954', 'FP_1963', 'FP_1973', 'FP_1974', 'FP_1976', 'FP_1977', 'FP_1980', 'FP_1984', 'FP_1986', 'FP_1993', 'FP_2007', 'FP_2030', 'FP_2034', 'FP_2040', 'FP_2041', 'FP_2045', 'FP_2046', 'FP_2047']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 250 | ['FP_144', 'FP_186', 'FP_217', 'FP_238', 'FP_298', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 1051 | ['NumRotatableBonds', 'RingCount', 'FP_1', 'FP_2', 'FP_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :    2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 1049 | ['FP_1', 'FP_2', 'FP_4', 'FP_5', 'FP_6', ...]\n",
      "\t2.0s = Fit runtime\n",
      "\t1439 features in original data used to generate 1439 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.85 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 118.04s of the 177.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0464875\n",
      "[2000]\tvalid_set's rmse: 0.0464654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0426541\n",
      "[2000]\tvalid_set's rmse: 0.0425489\n",
      "[3000]\tvalid_set's rmse: 0.0425367\n",
      "[4000]\tvalid_set's rmse: 0.0425361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.039\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.37s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 76.34s of the 135.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0436868\n",
      "[2000]\tvalid_set's rmse: 0.0435846\n",
      "[3000]\tvalid_set's rmse: 0.0435756\n",
      "[4000]\tvalid_set's rmse: 0.0435741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4464. Best iteration is:\n",
      "\t[4456]\tvalid_set's rmse: 0.0435739\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0403\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.76s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 34.22s of the 93.28s of remaining time.\n",
      "\t-0.0413\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.75s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 30.16s of the 89.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0409\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.45s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 10.33s of the 69.39s of remaining time.\n",
      "\t-0.0403\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.79s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 8.22s of the 67.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 7.76s of the 66.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\t-0.0419\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.25s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 0.26s of the 59.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.1s)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 177.10s of the 58.74s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.84, 'ExtraTreesMSE_BAG_L1': 0.08, 'CatBoost_BAG_L1': 0.04, 'XGBoost_BAG_L1': 0.04}\n",
      "\t-0.0389\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 58.73s of the 58.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0315462\n",
      "[2000]\tvalid_set's rmse: 0.0314916\n",
      "[3000]\tvalid_set's rmse: 0.0314861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3134. Best iteration is:\n",
      "\t[3076]\tvalid_set's rmse: 0.0314858\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0405\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.34s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 32.10s of the 32.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0411\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.71s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 12.11s of the 12.08s of remaining time.\n",
      "\t-0.0414\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.75s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 8.06s of the 8.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tcatboost/private/libs/options/json_helper.h:41: Can't parse parameter \"iterations\" with value: -1\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 389, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 868, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 350, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 355, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 391, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, num_cpus=num_cpus, num_gpus=num_gpus, **kwargs_fold)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\tabular\\models\\catboost\\catboost_model.py\", line 264, in _fit\n",
      "    self.model.fit(X, **fit_final_kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\catboost\\core.py\", line 2395, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\catboost\\core.py\", line 2321, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6601, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6623, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: catboost/private/libs/options/json_helper.h:41: Can't parse parameter \"iterations\" with value: -1\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 6.90s of the 6.87s of remaining time.\n",
      "\t-0.0402\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 5.07s of the 5.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 4.61s of the 4.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\t-0.0434\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.23s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 177.10s of the -0.01s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.88, 'CatBoost_BAG_L1': 0.08, 'XGBoost_BAG_L1': 0.04}\n",
      "\t-0.0389\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 179.2s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 313.4 rows/s (93 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Tc\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.9.23\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       16.49 GB / 31.11 GB (53.0%)\n",
      "Disk Space Avail:   15.51 GB / 1862.21 GB (0.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 60s of the 240s of remaining time (25%).\n",
      "\t\tContext path: \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Density\\ds_sub_fit\\sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Training for Target: Density | ‚è≥ Time Limit: 240s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Density\\ds_sub_fit\\sub_fit_ho\"\n",
      "Train Data Rows:    544\n",
      "Train Data Columns: 2438\n",
      "Label Column:       Density\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16899.40 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.88 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1248 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 800): ['FP_0', 'FP_3', 'FP_10', 'FP_12', 'FP_17', 'FP_18', 'FP_21', 'FP_22', 'FP_23', 'FP_27', 'FP_28', 'FP_30', 'FP_31', 'FP_35', 'FP_36', 'FP_40', 'FP_43', 'FP_44', 'FP_46', 'FP_48', 'FP_49', 'FP_51', 'FP_52', 'FP_53', 'FP_55', 'FP_63', 'FP_64', 'FP_66', 'FP_72', 'FP_73', 'FP_76', 'FP_77', 'FP_78', 'FP_79', 'FP_81', 'FP_82', 'FP_85', 'FP_86', 'FP_88', 'FP_89', 'FP_93', 'FP_96', 'FP_97', 'FP_98', 'FP_99', 'FP_106', 'FP_110', 'FP_111', 'FP_115', 'FP_122', 'FP_123', 'FP_124', 'FP_127', 'FP_131', 'FP_132', 'FP_134', 'FP_137', 'FP_138', 'FP_139', 'FP_140', 'FP_141', 'FP_143', 'FP_146', 'FP_148', 'FP_150', 'FP_153', 'FP_154', 'FP_157', 'FP_159', 'FP_160', 'FP_161', 'FP_164', 'FP_166', 'FP_169', 'FP_171', 'FP_176', 'FP_177', 'FP_178', 'FP_179', 'FP_183', 'FP_187', 'FP_188', 'FP_192', 'FP_198', 'FP_199', 'FP_207', 'FP_209', 'FP_211', 'FP_213', 'FP_214', 'FP_215', 'FP_217', 'FP_218', 'FP_219', 'FP_220', 'FP_221', 'FP_225', 'FP_228', 'FP_229', 'FP_230', 'FP_236', 'FP_239', 'FP_246', 'FP_254', 'FP_256', 'FP_257', 'FP_258', 'FP_259', 'FP_260', 'FP_261', 'FP_263', 'FP_265', 'FP_267', 'FP_268', 'FP_269', 'FP_275', 'FP_281', 'FP_284', 'FP_285', 'FP_286', 'FP_292', 'FP_293', 'FP_297', 'FP_299', 'FP_300', 'FP_301', 'FP_302', 'FP_309', 'FP_317', 'FP_318', 'FP_320', 'FP_321', 'FP_324', 'FP_326', 'FP_327', 'FP_328', 'FP_331', 'FP_332', 'FP_336', 'FP_337', 'FP_338', 'FP_340', 'FP_344', 'FP_346', 'FP_349', 'FP_355', 'FP_356', 'FP_358', 'FP_359', 'FP_363', 'FP_365', 'FP_367', 'FP_368', 'FP_369', 'FP_370', 'FP_372', 'FP_373', 'FP_376', 'FP_379', 'FP_384', 'FP_388', 'FP_391', 'FP_393', 'FP_395', 'FP_397', 'FP_398', 'FP_399', 'FP_402', 'FP_408', 'FP_415', 'FP_416', 'FP_417', 'FP_418', 'FP_423', 'FP_424', 'FP_426', 'FP_428', 'FP_429', 'FP_431', 'FP_433', 'FP_434', 'FP_435', 'FP_436', 'FP_437', 'FP_439', 'FP_443', 'FP_447', 'FP_449', 'FP_451', 'FP_453', 'FP_454', 'FP_455', 'FP_457', 'FP_461', 'FP_462', 'FP_465', 'FP_466', 'FP_467', 'FP_470', 'FP_475', 'FP_476', 'FP_477', 'FP_478', 'FP_480', 'FP_482', 'FP_483', 'FP_491', 'FP_494', 'FP_498', 'FP_505', 'FP_509', 'FP_510', 'FP_511', 'FP_512', 'FP_513', 'FP_515', 'FP_516', 'FP_521', 'FP_524', 'FP_525', 'FP_526', 'FP_527', 'FP_529', 'FP_533', 'FP_537', 'FP_542', 'FP_545', 'FP_547', 'FP_548', 'FP_549', 'FP_552', 'FP_553', 'FP_557', 'FP_558', 'FP_560', 'FP_566', 'FP_568', 'FP_569', 'FP_570', 'FP_571', 'FP_572', 'FP_574', 'FP_577', 'FP_578', 'FP_580', 'FP_582', 'FP_583', 'FP_586', 'FP_590', 'FP_593', 'FP_595', 'FP_596', 'FP_601', 'FP_605', 'FP_608', 'FP_611', 'FP_612', 'FP_613', 'FP_618', 'FP_623', 'FP_627', 'FP_630', 'FP_631', 'FP_633', 'FP_634', 'FP_635', 'FP_639', 'FP_642', 'FP_643', 'FP_645', 'FP_646', 'FP_648', 'FP_649', 'FP_651', 'FP_653', 'FP_657', 'FP_658', 'FP_660', 'FP_661', 'FP_664', 'FP_665', 'FP_668', 'FP_669', 'FP_671', 'FP_673', 'FP_676', 'FP_678', 'FP_681', 'FP_683', 'FP_684', 'FP_688', 'FP_696', 'FP_697', 'FP_700', 'FP_704', 'FP_706', 'FP_707', 'FP_708', 'FP_711', 'FP_712', 'FP_713', 'FP_719', 'FP_722', 'FP_723', 'FP_724', 'FP_726', 'FP_727', 'FP_733', 'FP_734', 'FP_735', 'FP_738', 'FP_740', 'FP_744', 'FP_751', 'FP_752', 'FP_756', 'FP_758', 'FP_761', 'FP_763', 'FP_764', 'FP_765', 'FP_767', 'FP_770', 'FP_771', 'FP_773', 'FP_776', 'FP_779', 'FP_780', 'FP_782', 'FP_784', 'FP_788', 'FP_789', 'FP_792', 'FP_793', 'FP_796', 'FP_797', 'FP_800', 'FP_804', 'FP_806', 'FP_808', 'FP_810', 'FP_811', 'FP_812', 'FP_815', 'FP_821', 'FP_823', 'FP_830', 'FP_840', 'FP_842', 'FP_846', 'FP_847', 'FP_848', 'FP_851', 'FP_853', 'FP_855', 'FP_856', 'FP_857', 'FP_858', 'FP_859', 'FP_860', 'FP_861', 'FP_865', 'FP_871', 'FP_873', 'FP_874', 'FP_883', 'FP_884', 'FP_885', 'FP_889', 'FP_894', 'FP_903', 'FP_904', 'FP_905', 'FP_907', 'FP_910', 'FP_911', 'FP_912', 'FP_918', 'FP_922', 'FP_923', 'FP_925', 'FP_928', 'FP_929', 'FP_930', 'FP_939', 'FP_942', 'FP_946', 'FP_947', 'FP_955', 'FP_957', 'FP_958', 'FP_966', 'FP_968', 'FP_970', 'FP_971', 'FP_979', 'FP_981', 'FP_983', 'FP_988', 'FP_990', 'FP_991', 'FP_993', 'FP_994', 'FP_995', 'FP_996', 'FP_999', 'FP_1001', 'FP_1002', 'FP_1006', 'FP_1007', 'FP_1008', 'FP_1009', 'FP_1010', 'FP_1015', 'FP_1016', 'FP_1018', 'FP_1020', 'FP_1021', 'FP_1023', 'FP_1025', 'FP_1030', 'FP_1033', 'FP_1037', 'FP_1040', 'FP_1042', 'FP_1044', 'FP_1048', 'FP_1050', 'FP_1054', 'FP_1067', 'FP_1068', 'FP_1073', 'FP_1075', 'FP_1078', 'FP_1079', 'FP_1080', 'FP_1092', 'FP_1093', 'FP_1094', 'FP_1095', 'FP_1102', 'FP_1105', 'FP_1106', 'FP_1111', 'FP_1115', 'FP_1117', 'FP_1122', 'FP_1124', 'FP_1125', 'FP_1131', 'FP_1132', 'FP_1134', 'FP_1137', 'FP_1138', 'FP_1144', 'FP_1146', 'FP_1149', 'FP_1150', 'FP_1151', 'FP_1158', 'FP_1165', 'FP_1167', 'FP_1170', 'FP_1171', 'FP_1172', 'FP_1177', 'FP_1179', 'FP_1181', 'FP_1183', 'FP_1186', 'FP_1188', 'FP_1189', 'FP_1192', 'FP_1193', 'FP_1194', 'FP_1197', 'FP_1200', 'FP_1201', 'FP_1202', 'FP_1204', 'FP_1205', 'FP_1207', 'FP_1208', 'FP_1211', 'FP_1214', 'FP_1215', 'FP_1219', 'FP_1221', 'FP_1223', 'FP_1231', 'FP_1234', 'FP_1242', 'FP_1245', 'FP_1246', 'FP_1247', 'FP_1248', 'FP_1250', 'FP_1251', 'FP_1253', 'FP_1254', 'FP_1255', 'FP_1262', 'FP_1275', 'FP_1277', 'FP_1278', 'FP_1281', 'FP_1289', 'FP_1291', 'FP_1293', 'FP_1296', 'FP_1299', 'FP_1300', 'FP_1301', 'FP_1305', 'FP_1306', 'FP_1307', 'FP_1311', 'FP_1315', 'FP_1316', 'FP_1318', 'FP_1320', 'FP_1324', 'FP_1329', 'FP_1334', 'FP_1336', 'FP_1340', 'FP_1341', 'FP_1342', 'FP_1343', 'FP_1345', 'FP_1346', 'FP_1348', 'FP_1358', 'FP_1361', 'FP_1364', 'FP_1367', 'FP_1372', 'FP_1374', 'FP_1375', 'FP_1376', 'FP_1379', 'FP_1387', 'FP_1390', 'FP_1394', 'FP_1395', 'FP_1396', 'FP_1397', 'FP_1401', 'FP_1406', 'FP_1409', 'FP_1412', 'FP_1414', 'FP_1418', 'FP_1421', 'FP_1422', 'FP_1425', 'FP_1426', 'FP_1427', 'FP_1433', 'FP_1434', 'FP_1435', 'FP_1439', 'FP_1442', 'FP_1449', 'FP_1451', 'FP_1458', 'FP_1463', 'FP_1468', 'FP_1469', 'FP_1471', 'FP_1477', 'FP_1478', 'FP_1481', 'FP_1482', 'FP_1483', 'FP_1484', 'FP_1488', 'FP_1491', 'FP_1492', 'FP_1493', 'FP_1494', 'FP_1495', 'FP_1496', 'FP_1499', 'FP_1501', 'FP_1503', 'FP_1504', 'FP_1505', 'FP_1512', 'FP_1513', 'FP_1514', 'FP_1515', 'FP_1517', 'FP_1521', 'FP_1523', 'FP_1525', 'FP_1528', 'FP_1529', 'FP_1532', 'FP_1537', 'FP_1538', 'FP_1541', 'FP_1542', 'FP_1546', 'FP_1547', 'FP_1548', 'FP_1549', 'FP_1552', 'FP_1553', 'FP_1554', 'FP_1558', 'FP_1560', 'FP_1562', 'FP_1563', 'FP_1565', 'FP_1568', 'FP_1570', 'FP_1576', 'FP_1577', 'FP_1578', 'FP_1579', 'FP_1580', 'FP_1584', 'FP_1585', 'FP_1586', 'FP_1587', 'FP_1590', 'FP_1593', 'FP_1595', 'FP_1596', 'FP_1600', 'FP_1605', 'FP_1606', 'FP_1608', 'FP_1613', 'FP_1614', 'FP_1615', 'FP_1616', 'FP_1619', 'FP_1622', 'FP_1623', 'FP_1625', 'FP_1626', 'FP_1627', 'FP_1629', 'FP_1632', 'FP_1635', 'FP_1636', 'FP_1637', 'FP_1639', 'FP_1641', 'FP_1646', 'FP_1651', 'FP_1656', 'FP_1657', 'FP_1659', 'FP_1660', 'FP_1663', 'FP_1664', 'FP_1667', 'FP_1672', 'FP_1674', 'FP_1675', 'FP_1676', 'FP_1677', 'FP_1678', 'FP_1679', 'FP_1684', 'FP_1689', 'FP_1690', 'FP_1694', 'FP_1696', 'FP_1700', 'FP_1701', 'FP_1702', 'FP_1703', 'FP_1704', 'FP_1705', 'FP_1709', 'FP_1711', 'FP_1720', 'FP_1727', 'FP_1731', 'FP_1736', 'FP_1738', 'FP_1740', 'FP_1744', 'FP_1746', 'FP_1748', 'FP_1749', 'FP_1753', 'FP_1759', 'FP_1762', 'FP_1765', 'FP_1766', 'FP_1767', 'FP_1768', 'FP_1769', 'FP_1776', 'FP_1779', 'FP_1780', 'FP_1782', 'FP_1786', 'FP_1790', 'FP_1792', 'FP_1796', 'FP_1797', 'FP_1800', 'FP_1801', 'FP_1802', 'FP_1805', 'FP_1807', 'FP_1808', 'FP_1812', 'FP_1813', 'FP_1815', 'FP_1817', 'FP_1821', 'FP_1822', 'FP_1828', 'FP_1829', 'FP_1830', 'FP_1833', 'FP_1835', 'FP_1837', 'FP_1838', 'FP_1841', 'FP_1848', 'FP_1849', 'FP_1850', 'FP_1851', 'FP_1857', 'FP_1858', 'FP_1864', 'FP_1865', 'FP_1867', 'FP_1868', 'FP_1870', 'FP_1874', 'FP_1875', 'FP_1877', 'FP_1881', 'FP_1884', 'FP_1887', 'FP_1889', 'FP_1890', 'FP_1892', 'FP_1894', 'FP_1897', 'FP_1902', 'FP_1903', 'FP_1906', 'FP_1907', 'FP_1910', 'FP_1921', 'FP_1922', 'FP_1925', 'FP_1929', 'FP_1931', 'FP_1932', 'FP_1933', 'FP_1935', 'FP_1937', 'FP_1939', 'FP_1941', 'FP_1944', 'FP_1945', 'FP_1946', 'FP_1959', 'FP_1960', 'FP_1962', 'FP_1965', 'FP_1967', 'FP_1969', 'FP_1971', 'FP_1988', 'FP_1994', 'FP_2000', 'FP_2001', 'FP_2003', 'FP_2005', 'FP_2008', 'FP_2010', 'FP_2011', 'FP_2012', 'FP_2014', 'FP_2015', 'FP_2016', 'FP_2019', 'FP_2020', 'FP_2021', 'FP_2022', 'FP_2026', 'FP_2027', 'FP_2028', 'FP_2029', 'FP_2031', 'FP_2035', 'FP_2036', 'FP_2038']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 281): ['FP_144', 'FP_155', 'FP_172', 'FP_238', 'FP_362', 'FP_377', 'FP_381', 'FP_405', 'FP_414', 'FP_432', 'FP_442', 'FP_458', 'FP_459', 'FP_468', 'FP_485', 'FP_528', 'FP_531', 'FP_535', 'FP_544', 'FP_550', 'FP_551', 'FP_554', 'FP_602', 'FP_625', 'FP_637', 'FP_644', 'FP_647', 'FP_662', 'FP_672', 'FP_731', 'FP_743', 'FP_750', 'FP_760', 'FP_772', 'FP_778', 'FP_801', 'FP_818', 'FP_819', 'FP_824', 'FP_826', 'FP_838', 'FP_839', 'FP_849', 'FP_850', 'FP_852', 'FP_866', 'FP_876', 'FP_891', 'FP_909', 'FP_916', 'FP_919', 'FP_920', 'FP_927', 'FP_936', 'FP_949', 'FP_951', 'FP_954', 'FP_956', 'FP_972', 'FP_977', 'FP_980', 'FP_982', 'FP_989', 'FP_1014', 'FP_1026', 'FP_1051', 'FP_1053', 'FP_1059', 'FP_1062', 'FP_1069', 'FP_1076', 'FP_1082', 'FP_1085', 'FP_1098', 'FP_1101', 'FP_1108', 'FP_1113', 'FP_1116', 'FP_1118', 'FP_1119', 'FP_1120', 'FP_1121', 'FP_1123', 'FP_1130', 'FP_1135', 'FP_1154', 'FP_1155', 'FP_1156', 'FP_1157', 'FP_1164', 'FP_1168', 'FP_1178', 'FP_1185', 'FP_1187', 'FP_1191', 'FP_1198', 'FP_1203', 'FP_1210', 'FP_1213', 'FP_1216', 'FP_1217', 'FP_1227', 'FP_1233', 'FP_1236', 'FP_1241', 'FP_1244', 'FP_1265', 'FP_1266', 'FP_1269', 'FP_1270', 'FP_1271', 'FP_1272', 'FP_1276', 'FP_1279', 'FP_1285', 'FP_1288', 'FP_1304', 'FP_1319', 'FP_1321', 'FP_1323', 'FP_1327', 'FP_1332', 'FP_1335', 'FP_1354', 'FP_1360', 'FP_1362', 'FP_1368', 'FP_1373', 'FP_1378', 'FP_1381', 'FP_1389', 'FP_1393', 'FP_1398', 'FP_1405', 'FP_1407', 'FP_1408', 'FP_1411', 'FP_1416', 'FP_1419', 'FP_1424', 'FP_1428', 'FP_1431', 'FP_1437', 'FP_1438', 'FP_1443', 'FP_1448', 'FP_1450', 'FP_1455', 'FP_1461', 'FP_1464', 'FP_1466', 'FP_1467', 'FP_1470', 'FP_1472', 'FP_1473', 'FP_1474', 'FP_1475', 'FP_1486', 'FP_1500', 'FP_1502', 'FP_1509', 'FP_1510', 'FP_1511', 'FP_1519', 'FP_1520', 'FP_1524', 'FP_1526', 'FP_1531', 'FP_1533', 'FP_1539', 'FP_1540', 'FP_1543', 'FP_1551', 'FP_1555', 'FP_1557', 'FP_1567', 'FP_1569', 'FP_1571', 'FP_1572', 'FP_1574', 'FP_1582', 'FP_1588', 'FP_1598', 'FP_1601', 'FP_1603', 'FP_1610', 'FP_1611', 'FP_1620', 'FP_1621', 'FP_1630', 'FP_1633', 'FP_1634', 'FP_1638', 'FP_1640', 'FP_1643', 'FP_1648', 'FP_1654', 'FP_1655', 'FP_1658', 'FP_1662', 'FP_1666', 'FP_1673', 'FP_1681', 'FP_1682', 'FP_1687', 'FP_1708', 'FP_1712', 'FP_1723', 'FP_1725', 'FP_1732', 'FP_1735', 'FP_1739', 'FP_1743', 'FP_1751', 'FP_1752', 'FP_1756', 'FP_1757', 'FP_1764', 'FP_1771', 'FP_1777', 'FP_1778', 'FP_1783', 'FP_1784', 'FP_1787', 'FP_1789', 'FP_1791', 'FP_1793', 'FP_1803', 'FP_1806', 'FP_1811', 'FP_1831', 'FP_1836', 'FP_1842', 'FP_1844', 'FP_1854', 'FP_1861', 'FP_1862', 'FP_1869', 'FP_1871', 'FP_1872', 'FP_1878', 'FP_1879', 'FP_1882', 'FP_1888', 'FP_1893', 'FP_1895', 'FP_1896', 'FP_1898', 'FP_1900', 'FP_1905', 'FP_1908', 'FP_1912', 'FP_1913', 'FP_1923', 'FP_1924', 'FP_1926', 'FP_1930', 'FP_1938', 'FP_1940', 'FP_1949', 'FP_1950', 'FP_1954', 'FP_1958', 'FP_1963', 'FP_1974', 'FP_1976', 'FP_1977', 'FP_1979', 'FP_1983', 'FP_1984', 'FP_1996', 'FP_2007', 'FP_2009', 'FP_2025', 'FP_2030', 'FP_2037', 'FP_2039', 'FP_2040', 'FP_2044', 'FP_2045', 'FP_2046']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 281 | ['FP_144', 'FP_155', 'FP_172', 'FP_238', 'FP_362', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 969 | ['NumRotatableBonds', 'RingCount', 'FP_1', 'FP_2', 'FP_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :   2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 967 | ['FP_1', 'FP_2', 'FP_4', 'FP_5', 'FP_6', ...]\n",
      "\t1.9s = Fit runtime\n",
      "\t1357 features in original data used to generate 1357 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.32 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.96s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 38.68s of the 58.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0497492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1337. Best iteration is:\n",
      "\t[1337]\tvalid_set's rmse: 0.0493294\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0488274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1409. Best iteration is:\n",
      "\t[1407]\tvalid_set's rmse: 0.0487642\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0722989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0537664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.080191\n",
      "[2000]\tvalid_set's rmse: 0.0799725\n",
      "[3000]\tvalid_set's rmse: 0.0799556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3940. Best iteration is:\n",
      "\t[3929]\tvalid_set's rmse: 0.0799511\n",
      "\t-0.067\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.64s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2.65s of the 22.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's rmse: 0.136017\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1.90s of the 21.25s of remaining time.\n",
      "\t-0.0798\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.79s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 58.04s of the 18.19s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.067\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 18.18s of the 18.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 542. Best iteration is:\n",
      "\t[262]\tvalid_set's rmse: 0.0772435\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 549. Best iteration is:\n",
      "\t[276]\tvalid_set's rmse: 0.0563148\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 634. Best iteration is:\n",
      "\t[634]\tvalid_set's rmse: 0.0694548\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 694. Best iteration is:\n",
      "\t[694]\tvalid_set's rmse: 0.0544714\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0679\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.51s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 3.43s of the 3.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's rmse: 0.132002\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 2.58s of the 2.57s of remaining time.\n",
      "\t-0.0709\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.74s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 58.04s of the -0.45s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.067\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 60.47s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1655.6 rows/s (68 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Density\\ds_sub_fit\\sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       LightGBMXT_BAG_L2      -0.091674  -0.067867  root_mean_squared_error        0.312031       0.179011  52.937692                 0.102019                0.038004          14.509250            2       True          4\n",
      "1       LightGBMXT_BAG_L1      -0.092734  -0.067008  root_mean_squared_error        0.136005       0.041011  35.642913                 0.136005                0.041011          35.642913            1       True          1\n",
      "2     WeightedEnsemble_L3      -0.092734  -0.067008  root_mean_squared_error        0.144005       0.041511  35.645410                 0.008000                0.000500           0.002497            3       True          6\n",
      "3     WeightedEnsemble_L2      -0.092734  -0.067008  root_mean_squared_error        0.144504       0.041511  35.645913                 0.008499                0.000500           0.003000            2       True          3\n",
      "4  RandomForestMSE_BAG_L2      -0.098869  -0.070866  root_mean_squared_error        0.269524       0.252011  41.171458                 0.059512                0.111004           2.743016            2       True          5\n",
      "5  RandomForestMSE_BAG_L1      -0.100104  -0.079773  root_mean_squared_error        0.074007       0.099996   2.785530                 0.074007                0.099996           2.785530            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t61s\t = DyStack   runtime |\t179s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 179s\n",
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Density\"\n",
      "Train Data Rows:    613\n",
      "Train Data Columns: 2438\n",
      "Label Column:       Density\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16902.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.12 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1318 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 730): ['FP_0', 'FP_3', 'FP_10', 'FP_17', 'FP_21', 'FP_22', 'FP_23', 'FP_27', 'FP_28', 'FP_30', 'FP_31', 'FP_35', 'FP_36', 'FP_40', 'FP_43', 'FP_44', 'FP_46', 'FP_48', 'FP_49', 'FP_51', 'FP_53', 'FP_55', 'FP_63', 'FP_64', 'FP_66', 'FP_72', 'FP_73', 'FP_76', 'FP_77', 'FP_78', 'FP_79', 'FP_81', 'FP_82', 'FP_85', 'FP_86', 'FP_88', 'FP_93', 'FP_96', 'FP_97', 'FP_98', 'FP_106', 'FP_110', 'FP_111', 'FP_115', 'FP_122', 'FP_123', 'FP_124', 'FP_127', 'FP_131', 'FP_132', 'FP_134', 'FP_139', 'FP_141', 'FP_143', 'FP_146', 'FP_148', 'FP_150', 'FP_153', 'FP_154', 'FP_157', 'FP_159', 'FP_160', 'FP_164', 'FP_166', 'FP_169', 'FP_171', 'FP_176', 'FP_177', 'FP_178', 'FP_179', 'FP_183', 'FP_187', 'FP_188', 'FP_192', 'FP_198', 'FP_199', 'FP_207', 'FP_209', 'FP_211', 'FP_214', 'FP_215', 'FP_217', 'FP_218', 'FP_219', 'FP_220', 'FP_221', 'FP_225', 'FP_228', 'FP_229', 'FP_230', 'FP_236', 'FP_239', 'FP_246', 'FP_254', 'FP_256', 'FP_257', 'FP_258', 'FP_259', 'FP_260', 'FP_261', 'FP_263', 'FP_265', 'FP_267', 'FP_268', 'FP_269', 'FP_275', 'FP_284', 'FP_285', 'FP_286', 'FP_292', 'FP_293', 'FP_297', 'FP_299', 'FP_300', 'FP_301', 'FP_302', 'FP_309', 'FP_317', 'FP_318', 'FP_320', 'FP_321', 'FP_324', 'FP_326', 'FP_327', 'FP_331', 'FP_332', 'FP_336', 'FP_337', 'FP_338', 'FP_340', 'FP_344', 'FP_346', 'FP_349', 'FP_355', 'FP_356', 'FP_358', 'FP_359', 'FP_363', 'FP_365', 'FP_368', 'FP_369', 'FP_370', 'FP_372', 'FP_373', 'FP_376', 'FP_379', 'FP_384', 'FP_388', 'FP_391', 'FP_393', 'FP_395', 'FP_397', 'FP_398', 'FP_399', 'FP_402', 'FP_408', 'FP_415', 'FP_416', 'FP_417', 'FP_418', 'FP_423', 'FP_426', 'FP_428', 'FP_429', 'FP_431', 'FP_434', 'FP_435', 'FP_436', 'FP_437', 'FP_439', 'FP_443', 'FP_447', 'FP_449', 'FP_451', 'FP_453', 'FP_454', 'FP_455', 'FP_457', 'FP_462', 'FP_465', 'FP_466', 'FP_467', 'FP_470', 'FP_475', 'FP_476', 'FP_477', 'FP_478', 'FP_480', 'FP_482', 'FP_483', 'FP_491', 'FP_494', 'FP_498', 'FP_505', 'FP_509', 'FP_510', 'FP_511', 'FP_512', 'FP_513', 'FP_515', 'FP_516', 'FP_521', 'FP_524', 'FP_526', 'FP_527', 'FP_529', 'FP_533', 'FP_537', 'FP_542', 'FP_545', 'FP_547', 'FP_548', 'FP_552', 'FP_557', 'FP_558', 'FP_560', 'FP_566', 'FP_568', 'FP_569', 'FP_571', 'FP_572', 'FP_574', 'FP_577', 'FP_578', 'FP_580', 'FP_583', 'FP_586', 'FP_590', 'FP_593', 'FP_595', 'FP_596', 'FP_601', 'FP_605', 'FP_608', 'FP_611', 'FP_612', 'FP_613', 'FP_618', 'FP_623', 'FP_627', 'FP_630', 'FP_631', 'FP_633', 'FP_634', 'FP_635', 'FP_639', 'FP_642', 'FP_643', 'FP_646', 'FP_648', 'FP_649', 'FP_651', 'FP_653', 'FP_657', 'FP_658', 'FP_660', 'FP_661', 'FP_664', 'FP_668', 'FP_669', 'FP_671', 'FP_673', 'FP_676', 'FP_678', 'FP_681', 'FP_683', 'FP_684', 'FP_688', 'FP_696', 'FP_697', 'FP_700', 'FP_704', 'FP_706', 'FP_707', 'FP_708', 'FP_711', 'FP_713', 'FP_719', 'FP_722', 'FP_723', 'FP_724', 'FP_726', 'FP_727', 'FP_733', 'FP_734', 'FP_735', 'FP_738', 'FP_740', 'FP_744', 'FP_751', 'FP_752', 'FP_756', 'FP_758', 'FP_761', 'FP_763', 'FP_765', 'FP_767', 'FP_770', 'FP_771', 'FP_773', 'FP_776', 'FP_779', 'FP_780', 'FP_784', 'FP_788', 'FP_789', 'FP_792', 'FP_793', 'FP_796', 'FP_797', 'FP_800', 'FP_804', 'FP_806', 'FP_808', 'FP_810', 'FP_812', 'FP_815', 'FP_823', 'FP_830', 'FP_840', 'FP_842', 'FP_846', 'FP_847', 'FP_848', 'FP_851', 'FP_853', 'FP_855', 'FP_856', 'FP_857', 'FP_858', 'FP_859', 'FP_860', 'FP_861', 'FP_865', 'FP_871', 'FP_873', 'FP_874', 'FP_883', 'FP_884', 'FP_889', 'FP_894', 'FP_903', 'FP_904', 'FP_910', 'FP_911', 'FP_912', 'FP_918', 'FP_922', 'FP_923', 'FP_925', 'FP_928', 'FP_929', 'FP_930', 'FP_939', 'FP_942', 'FP_946', 'FP_947', 'FP_955', 'FP_957', 'FP_958', 'FP_966', 'FP_968', 'FP_970', 'FP_971', 'FP_979', 'FP_983', 'FP_988', 'FP_990', 'FP_991', 'FP_993', 'FP_995', 'FP_996', 'FP_999', 'FP_1001', 'FP_1002', 'FP_1006', 'FP_1007', 'FP_1008', 'FP_1009', 'FP_1010', 'FP_1015', 'FP_1016', 'FP_1018', 'FP_1020', 'FP_1021', 'FP_1023', 'FP_1025', 'FP_1030', 'FP_1033', 'FP_1037', 'FP_1040', 'FP_1042', 'FP_1044', 'FP_1048', 'FP_1050', 'FP_1054', 'FP_1067', 'FP_1068', 'FP_1073', 'FP_1078', 'FP_1079', 'FP_1080', 'FP_1092', 'FP_1093', 'FP_1094', 'FP_1095', 'FP_1102', 'FP_1105', 'FP_1106', 'FP_1111', 'FP_1115', 'FP_1117', 'FP_1122', 'FP_1124', 'FP_1125', 'FP_1131', 'FP_1132', 'FP_1134', 'FP_1137', 'FP_1144', 'FP_1146', 'FP_1149', 'FP_1150', 'FP_1151', 'FP_1158', 'FP_1165', 'FP_1167', 'FP_1172', 'FP_1177', 'FP_1179', 'FP_1181', 'FP_1183', 'FP_1186', 'FP_1188', 'FP_1192', 'FP_1193', 'FP_1194', 'FP_1197', 'FP_1200', 'FP_1201', 'FP_1204', 'FP_1205', 'FP_1207', 'FP_1208', 'FP_1214', 'FP_1215', 'FP_1219', 'FP_1221', 'FP_1223', 'FP_1231', 'FP_1234', 'FP_1242', 'FP_1245', 'FP_1246', 'FP_1247', 'FP_1248', 'FP_1250', 'FP_1251', 'FP_1253', 'FP_1254', 'FP_1262', 'FP_1275', 'FP_1277', 'FP_1278', 'FP_1281', 'FP_1289', 'FP_1291', 'FP_1293', 'FP_1296', 'FP_1299', 'FP_1300', 'FP_1301', 'FP_1305', 'FP_1306', 'FP_1307', 'FP_1311', 'FP_1315', 'FP_1316', 'FP_1318', 'FP_1320', 'FP_1324', 'FP_1329', 'FP_1334', 'FP_1336', 'FP_1340', 'FP_1341', 'FP_1342', 'FP_1343', 'FP_1345', 'FP_1348', 'FP_1358', 'FP_1361', 'FP_1364', 'FP_1367', 'FP_1374', 'FP_1375', 'FP_1376', 'FP_1387', 'FP_1390', 'FP_1394', 'FP_1395', 'FP_1397', 'FP_1401', 'FP_1406', 'FP_1409', 'FP_1412', 'FP_1414', 'FP_1418', 'FP_1421', 'FP_1422', 'FP_1425', 'FP_1426', 'FP_1433', 'FP_1434', 'FP_1435', 'FP_1439', 'FP_1442', 'FP_1449', 'FP_1451', 'FP_1458', 'FP_1463', 'FP_1469', 'FP_1471', 'FP_1477', 'FP_1481', 'FP_1483', 'FP_1484', 'FP_1488', 'FP_1491', 'FP_1492', 'FP_1493', 'FP_1494', 'FP_1496', 'FP_1499', 'FP_1501', 'FP_1503', 'FP_1504', 'FP_1505', 'FP_1512', 'FP_1514', 'FP_1515', 'FP_1517', 'FP_1521', 'FP_1523', 'FP_1525', 'FP_1528', 'FP_1529', 'FP_1532', 'FP_1537', 'FP_1538', 'FP_1541', 'FP_1546', 'FP_1547', 'FP_1548', 'FP_1549', 'FP_1552', 'FP_1553', 'FP_1554', 'FP_1558', 'FP_1560', 'FP_1562', 'FP_1563', 'FP_1568', 'FP_1570', 'FP_1576', 'FP_1577', 'FP_1580', 'FP_1584', 'FP_1585', 'FP_1586', 'FP_1587', 'FP_1593', 'FP_1595', 'FP_1596', 'FP_1600', 'FP_1605', 'FP_1606', 'FP_1608', 'FP_1613', 'FP_1614', 'FP_1615', 'FP_1616', 'FP_1619', 'FP_1622', 'FP_1623', 'FP_1625', 'FP_1627', 'FP_1629', 'FP_1632', 'FP_1635', 'FP_1636', 'FP_1637', 'FP_1639', 'FP_1641', 'FP_1646', 'FP_1651', 'FP_1657', 'FP_1659', 'FP_1660', 'FP_1663', 'FP_1664', 'FP_1667', 'FP_1672', 'FP_1674', 'FP_1676', 'FP_1677', 'FP_1678', 'FP_1679', 'FP_1689', 'FP_1690', 'FP_1694', 'FP_1696', 'FP_1701', 'FP_1702', 'FP_1703', 'FP_1704', 'FP_1705', 'FP_1709', 'FP_1711', 'FP_1727', 'FP_1731', 'FP_1736', 'FP_1738', 'FP_1740', 'FP_1744', 'FP_1746', 'FP_1748', 'FP_1749', 'FP_1759', 'FP_1762', 'FP_1765', 'FP_1766', 'FP_1767', 'FP_1768', 'FP_1769', 'FP_1776', 'FP_1779', 'FP_1780', 'FP_1782', 'FP_1786', 'FP_1790', 'FP_1792', 'FP_1796', 'FP_1797', 'FP_1800', 'FP_1802', 'FP_1805', 'FP_1807', 'FP_1808', 'FP_1812', 'FP_1813', 'FP_1815', 'FP_1817', 'FP_1821', 'FP_1822', 'FP_1828', 'FP_1829', 'FP_1830', 'FP_1833', 'FP_1835', 'FP_1837', 'FP_1838', 'FP_1841', 'FP_1848', 'FP_1849', 'FP_1851', 'FP_1857', 'FP_1858', 'FP_1864', 'FP_1865', 'FP_1867', 'FP_1868', 'FP_1870', 'FP_1874', 'FP_1875', 'FP_1877', 'FP_1881', 'FP_1884', 'FP_1887', 'FP_1889', 'FP_1890', 'FP_1892', 'FP_1894', 'FP_1897', 'FP_1902', 'FP_1903', 'FP_1906', 'FP_1907', 'FP_1910', 'FP_1921', 'FP_1922', 'FP_1925', 'FP_1929', 'FP_1931', 'FP_1932', 'FP_1933', 'FP_1935', 'FP_1937', 'FP_1939', 'FP_1941', 'FP_1944', 'FP_1945', 'FP_1946', 'FP_1959', 'FP_1960', 'FP_1962', 'FP_1965', 'FP_1967', 'FP_1971', 'FP_1988', 'FP_1994', 'FP_2000', 'FP_2001', 'FP_2003', 'FP_2005', 'FP_2008', 'FP_2010', 'FP_2011', 'FP_2012', 'FP_2014', 'FP_2015', 'FP_2016', 'FP_2019', 'FP_2021', 'FP_2026', 'FP_2028', 'FP_2029', 'FP_2031', 'FP_2035', 'FP_2036']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 274): ['FP_144', 'FP_155', 'FP_238', 'FP_377', 'FP_381', 'FP_405', 'FP_414', 'FP_432', 'FP_433', 'FP_442', 'FP_458', 'FP_459', 'FP_468', 'FP_485', 'FP_525', 'FP_531', 'FP_544', 'FP_550', 'FP_551', 'FP_554', 'FP_625', 'FP_644', 'FP_647', 'FP_662', 'FP_665', 'FP_712', 'FP_731', 'FP_743', 'FP_750', 'FP_760', 'FP_764', 'FP_772', 'FP_782', 'FP_818', 'FP_819', 'FP_821', 'FP_824', 'FP_826', 'FP_838', 'FP_839', 'FP_850', 'FP_885', 'FP_905', 'FP_907', 'FP_909', 'FP_916', 'FP_919', 'FP_920', 'FP_927', 'FP_936', 'FP_949', 'FP_951', 'FP_954', 'FP_956', 'FP_972', 'FP_977', 'FP_980', 'FP_982', 'FP_989', 'FP_1014', 'FP_1026', 'FP_1051', 'FP_1059', 'FP_1062', 'FP_1069', 'FP_1076', 'FP_1082', 'FP_1085', 'FP_1098', 'FP_1101', 'FP_1108', 'FP_1118', 'FP_1119', 'FP_1121', 'FP_1123', 'FP_1130', 'FP_1135', 'FP_1138', 'FP_1154', 'FP_1155', 'FP_1156', 'FP_1157', 'FP_1164', 'FP_1171', 'FP_1178', 'FP_1187', 'FP_1189', 'FP_1191', 'FP_1198', 'FP_1203', 'FP_1210', 'FP_1216', 'FP_1217', 'FP_1227', 'FP_1233', 'FP_1236', 'FP_1241', 'FP_1255', 'FP_1265', 'FP_1266', 'FP_1269', 'FP_1270', 'FP_1272', 'FP_1276', 'FP_1279', 'FP_1285', 'FP_1288', 'FP_1304', 'FP_1319', 'FP_1321', 'FP_1327', 'FP_1332', 'FP_1346', 'FP_1360', 'FP_1362', 'FP_1372', 'FP_1373', 'FP_1378', 'FP_1379', 'FP_1381', 'FP_1389', 'FP_1393', 'FP_1396', 'FP_1398', 'FP_1407', 'FP_1408', 'FP_1411', 'FP_1416', 'FP_1419', 'FP_1424', 'FP_1427', 'FP_1428', 'FP_1431', 'FP_1437', 'FP_1438', 'FP_1443', 'FP_1448', 'FP_1450', 'FP_1455', 'FP_1461', 'FP_1464', 'FP_1466', 'FP_1467', 'FP_1468', 'FP_1470', 'FP_1472', 'FP_1473', 'FP_1474', 'FP_1475', 'FP_1486', 'FP_1495', 'FP_1502', 'FP_1510', 'FP_1511', 'FP_1513', 'FP_1519', 'FP_1520', 'FP_1524', 'FP_1526', 'FP_1531', 'FP_1539', 'FP_1540', 'FP_1543', 'FP_1551', 'FP_1555', 'FP_1557', 'FP_1565', 'FP_1567', 'FP_1569', 'FP_1571', 'FP_1572', 'FP_1574', 'FP_1578', 'FP_1579', 'FP_1582', 'FP_1588', 'FP_1590', 'FP_1603', 'FP_1610', 'FP_1611', 'FP_1620', 'FP_1621', 'FP_1626', 'FP_1630', 'FP_1633', 'FP_1634', 'FP_1640', 'FP_1648', 'FP_1654', 'FP_1655', 'FP_1656', 'FP_1658', 'FP_1662', 'FP_1666', 'FP_1673', 'FP_1675', 'FP_1681', 'FP_1682', 'FP_1687', 'FP_1700', 'FP_1708', 'FP_1712', 'FP_1720', 'FP_1725', 'FP_1732', 'FP_1735', 'FP_1739', 'FP_1743', 'FP_1751', 'FP_1752', 'FP_1753', 'FP_1756', 'FP_1757', 'FP_1764', 'FP_1771', 'FP_1777', 'FP_1783', 'FP_1787', 'FP_1789', 'FP_1791', 'FP_1793', 'FP_1801', 'FP_1803', 'FP_1806', 'FP_1811', 'FP_1831', 'FP_1836', 'FP_1842', 'FP_1844', 'FP_1854', 'FP_1861', 'FP_1862', 'FP_1869', 'FP_1871', 'FP_1872', 'FP_1882', 'FP_1888', 'FP_1895', 'FP_1896', 'FP_1898', 'FP_1900', 'FP_1905', 'FP_1908', 'FP_1912', 'FP_1913', 'FP_1923', 'FP_1924', 'FP_1926', 'FP_1938', 'FP_1940', 'FP_1949', 'FP_1950', 'FP_1954', 'FP_1958', 'FP_1963', 'FP_1969', 'FP_1974', 'FP_1976', 'FP_1977', 'FP_1979', 'FP_1983', 'FP_1984', 'FP_2007', 'FP_2009', 'FP_2020', 'FP_2022', 'FP_2030', 'FP_2037', 'FP_2038', 'FP_2039', 'FP_2040', 'FP_2044', 'FP_2045', 'FP_2046']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 274 | ['FP_144', 'FP_155', 'FP_238', 'FP_377', 'FP_381', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 1046 | ['NumRotatableBonds', 'RingCount', 'FP_1', 'FP_2', 'FP_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :    2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 1044 | ['FP_1', 'FP_2', 'FP_4', 'FP_5', 'FP_6', ...]\n",
      "\t2.1s = Fit runtime\n",
      "\t1434 features in original data used to generate 1434 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.54 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 117.74s of the 176.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.100371\n",
      "[2000]\tvalid_set's rmse: 0.100212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0838172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0498368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0614437\n",
      "[2000]\tvalid_set's rmse: 0.0610556\n",
      "[3000]\tvalid_set's rmse: 0.0610126\n",
      "[4000]\tvalid_set's rmse: 0.0610075\n",
      "[5000]\tvalid_set's rmse: 0.0610064\n",
      "[6000]\tvalid_set's rmse: 0.0610062\n",
      "[7000]\tvalid_set's rmse: 0.0610062\n",
      "[8000]\tvalid_set's rmse: 0.0610061\n",
      "[9000]\tvalid_set's rmse: 0.0610061\n",
      "[10000]\tvalid_set's rmse: 0.0610061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0694\t = Validation score   (-root_mean_squared_error)\n",
      "\t61.64s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 55.61s of the 114.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.10245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1206. Best iteration is:\n",
      "\t[1200]\tvalid_set's rmse: 0.102365\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0692918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1875. Best iteration is:\n",
      "\t[1872]\tvalid_set's rmse: 0.0691861\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0726\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.14s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 25.12s of the 84.03s of remaining time.\n",
      "\t-0.0821\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.08s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 21.74s of the 80.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0761\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.91s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1.45s of the 60.36s of remaining time.\n",
      "\t-0.0766\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.45s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 176.66s of the 58.58s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.909, 'ExtraTreesMSE_BAG_L1': 0.091}\n",
      "\t-0.0693\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 58.57s of the 58.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.0574507\n",
      "[2000]\tvalid_set's rmse: 0.0572917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0687\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.98s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 36.32s of the 36.30s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-0.0705\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.45s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 19.61s of the 19.59s of remaining time.\n",
      "\t-0.073\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.09s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 16.19s of the 16.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0773\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.96s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 2.85s of the 2.84s of remaining time.\n",
      "\t-0.0702\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.68s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 176.66s of the 0.79s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.684, 'ExtraTreesMSE_BAG_L2': 0.316}\n",
      "\t-0.0683\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 178.02s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 227.4 rows/s (77 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Density\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.9.23\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       16.50 GB / 31.11 GB (53.0%)\n",
      "Disk Space Avail:   15.62 GB / 1862.21 GB (0.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 60s of the 240s of remaining time (25%).\n",
      "\t\tContext path: \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Rg\\ds_sub_fit\\sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Training for Target: Rg | ‚è≥ Time Limit: 240s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Rg\\ds_sub_fit\\sub_fit_ho\"\n",
      "Train Data Rows:    545\n",
      "Train Data Columns: 2438\n",
      "Label Column:       Rg\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16894.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.89 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1260 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 788): ['FP_0', 'FP_3', 'FP_10', 'FP_12', 'FP_17', 'FP_18', 'FP_21', 'FP_22', 'FP_23', 'FP_27', 'FP_28', 'FP_30', 'FP_31', 'FP_35', 'FP_36', 'FP_40', 'FP_43', 'FP_44', 'FP_46', 'FP_48', 'FP_49', 'FP_50', 'FP_51', 'FP_53', 'FP_55', 'FP_60', 'FP_63', 'FP_64', 'FP_66', 'FP_72', 'FP_73', 'FP_76', 'FP_77', 'FP_78', 'FP_79', 'FP_81', 'FP_82', 'FP_85', 'FP_86', 'FP_88', 'FP_93', 'FP_96', 'FP_97', 'FP_98', 'FP_106', 'FP_110', 'FP_111', 'FP_115', 'FP_122', 'FP_123', 'FP_124', 'FP_127', 'FP_131', 'FP_132', 'FP_134', 'FP_138', 'FP_139', 'FP_141', 'FP_143', 'FP_144', 'FP_146', 'FP_148', 'FP_150', 'FP_153', 'FP_154', 'FP_157', 'FP_159', 'FP_160', 'FP_164', 'FP_166', 'FP_169', 'FP_171', 'FP_176', 'FP_177', 'FP_178', 'FP_179', 'FP_183', 'FP_187', 'FP_188', 'FP_192', 'FP_198', 'FP_199', 'FP_207', 'FP_209', 'FP_211', 'FP_214', 'FP_215', 'FP_217', 'FP_218', 'FP_219', 'FP_220', 'FP_221', 'FP_225', 'FP_228', 'FP_229', 'FP_230', 'FP_236', 'FP_239', 'FP_246', 'FP_254', 'FP_256', 'FP_257', 'FP_258', 'FP_259', 'FP_260', 'FP_261', 'FP_263', 'FP_265', 'FP_267', 'FP_268', 'FP_269', 'FP_275', 'FP_281', 'FP_284', 'FP_285', 'FP_286', 'FP_292', 'FP_293', 'FP_297', 'FP_298', 'FP_299', 'FP_300', 'FP_301', 'FP_302', 'FP_309', 'FP_317', 'FP_318', 'FP_320', 'FP_321', 'FP_324', 'FP_326', 'FP_327', 'FP_328', 'FP_331', 'FP_332', 'FP_336', 'FP_337', 'FP_338', 'FP_340', 'FP_344', 'FP_346', 'FP_349', 'FP_355', 'FP_356', 'FP_358', 'FP_359', 'FP_363', 'FP_365', 'FP_367', 'FP_368', 'FP_369', 'FP_370', 'FP_372', 'FP_373', 'FP_376', 'FP_379', 'FP_384', 'FP_387', 'FP_388', 'FP_391', 'FP_393', 'FP_395', 'FP_397', 'FP_398', 'FP_399', 'FP_402', 'FP_408', 'FP_415', 'FP_416', 'FP_417', 'FP_418', 'FP_423', 'FP_424', 'FP_426', 'FP_428', 'FP_429', 'FP_431', 'FP_433', 'FP_434', 'FP_435', 'FP_436', 'FP_437', 'FP_439', 'FP_443', 'FP_447', 'FP_449', 'FP_451', 'FP_453', 'FP_454', 'FP_455', 'FP_457', 'FP_462', 'FP_464', 'FP_465', 'FP_466', 'FP_467', 'FP_470', 'FP_475', 'FP_476', 'FP_477', 'FP_478', 'FP_480', 'FP_482', 'FP_483', 'FP_491', 'FP_494', 'FP_498', 'FP_505', 'FP_509', 'FP_510', 'FP_511', 'FP_512', 'FP_513', 'FP_515', 'FP_516', 'FP_521', 'FP_524', 'FP_526', 'FP_527', 'FP_529', 'FP_533', 'FP_537', 'FP_542', 'FP_545', 'FP_547', 'FP_548', 'FP_552', 'FP_553', 'FP_557', 'FP_558', 'FP_560', 'FP_566', 'FP_568', 'FP_569', 'FP_571', 'FP_572', 'FP_574', 'FP_577', 'FP_578', 'FP_580', 'FP_582', 'FP_583', 'FP_586', 'FP_590', 'FP_593', 'FP_595', 'FP_596', 'FP_601', 'FP_602', 'FP_605', 'FP_608', 'FP_611', 'FP_612', 'FP_613', 'FP_618', 'FP_623', 'FP_627', 'FP_630', 'FP_631', 'FP_633', 'FP_634', 'FP_635', 'FP_639', 'FP_642', 'FP_643', 'FP_646', 'FP_648', 'FP_649', 'FP_651', 'FP_653', 'FP_657', 'FP_658', 'FP_660', 'FP_661', 'FP_663', 'FP_664', 'FP_668', 'FP_669', 'FP_671', 'FP_673', 'FP_676', 'FP_678', 'FP_681', 'FP_683', 'FP_684', 'FP_688', 'FP_690', 'FP_696', 'FP_697', 'FP_700', 'FP_704', 'FP_706', 'FP_707', 'FP_708', 'FP_711', 'FP_712', 'FP_713', 'FP_719', 'FP_722', 'FP_723', 'FP_724', 'FP_726', 'FP_727', 'FP_733', 'FP_734', 'FP_735', 'FP_738', 'FP_740', 'FP_744', 'FP_751', 'FP_752', 'FP_756', 'FP_758', 'FP_761', 'FP_763', 'FP_765', 'FP_767', 'FP_770', 'FP_771', 'FP_773', 'FP_776', 'FP_778', 'FP_779', 'FP_780', 'FP_782', 'FP_784', 'FP_788', 'FP_789', 'FP_792', 'FP_793', 'FP_796', 'FP_797', 'FP_800', 'FP_804', 'FP_806', 'FP_808', 'FP_810', 'FP_812', 'FP_815', 'FP_823', 'FP_829', 'FP_830', 'FP_840', 'FP_842', 'FP_846', 'FP_847', 'FP_848', 'FP_851', 'FP_853', 'FP_855', 'FP_856', 'FP_857', 'FP_858', 'FP_859', 'FP_860', 'FP_861', 'FP_865', 'FP_871', 'FP_873', 'FP_874', 'FP_883', 'FP_884', 'FP_889', 'FP_891', 'FP_894', 'FP_903', 'FP_904', 'FP_910', 'FP_911', 'FP_912', 'FP_918', 'FP_922', 'FP_923', 'FP_925', 'FP_928', 'FP_929', 'FP_930', 'FP_939', 'FP_942', 'FP_944', 'FP_946', 'FP_947', 'FP_954', 'FP_955', 'FP_957', 'FP_958', 'FP_966', 'FP_968', 'FP_970', 'FP_971', 'FP_976', 'FP_979', 'FP_983', 'FP_988', 'FP_990', 'FP_991', 'FP_993', 'FP_994', 'FP_995', 'FP_996', 'FP_999', 'FP_1001', 'FP_1002', 'FP_1006', 'FP_1007', 'FP_1008', 'FP_1009', 'FP_1010', 'FP_1015', 'FP_1016', 'FP_1018', 'FP_1020', 'FP_1021', 'FP_1023', 'FP_1025', 'FP_1030', 'FP_1033', 'FP_1036', 'FP_1037', 'FP_1040', 'FP_1042', 'FP_1044', 'FP_1048', 'FP_1050', 'FP_1054', 'FP_1067', 'FP_1068', 'FP_1073', 'FP_1078', 'FP_1079', 'FP_1080', 'FP_1082', 'FP_1092', 'FP_1093', 'FP_1094', 'FP_1095', 'FP_1102', 'FP_1105', 'FP_1106', 'FP_1111', 'FP_1115', 'FP_1117', 'FP_1122', 'FP_1124', 'FP_1125', 'FP_1130', 'FP_1131', 'FP_1132', 'FP_1134', 'FP_1137', 'FP_1144', 'FP_1146', 'FP_1149', 'FP_1150', 'FP_1151', 'FP_1158', 'FP_1165', 'FP_1167', 'FP_1172', 'FP_1177', 'FP_1179', 'FP_1181', 'FP_1183', 'FP_1186', 'FP_1188', 'FP_1189', 'FP_1191', 'FP_1192', 'FP_1193', 'FP_1194', 'FP_1197', 'FP_1200', 'FP_1201', 'FP_1204', 'FP_1205', 'FP_1207', 'FP_1208', 'FP_1214', 'FP_1215', 'FP_1219', 'FP_1221', 'FP_1223', 'FP_1231', 'FP_1234', 'FP_1242', 'FP_1245', 'FP_1246', 'FP_1247', 'FP_1248', 'FP_1250', 'FP_1251', 'FP_1253', 'FP_1254', 'FP_1256', 'FP_1262', 'FP_1275', 'FP_1276', 'FP_1277', 'FP_1278', 'FP_1281', 'FP_1289', 'FP_1291', 'FP_1293', 'FP_1296', 'FP_1299', 'FP_1300', 'FP_1301', 'FP_1305', 'FP_1306', 'FP_1307', 'FP_1311', 'FP_1315', 'FP_1316', 'FP_1318', 'FP_1320', 'FP_1324', 'FP_1329', 'FP_1334', 'FP_1336', 'FP_1340', 'FP_1341', 'FP_1342', 'FP_1343', 'FP_1345', 'FP_1346', 'FP_1348', 'FP_1358', 'FP_1361', 'FP_1363', 'FP_1364', 'FP_1367', 'FP_1372', 'FP_1374', 'FP_1375', 'FP_1376', 'FP_1377', 'FP_1387', 'FP_1390', 'FP_1394', 'FP_1395', 'FP_1396', 'FP_1397', 'FP_1401', 'FP_1406', 'FP_1409', 'FP_1412', 'FP_1414', 'FP_1418', 'FP_1421', 'FP_1422', 'FP_1425', 'FP_1426', 'FP_1427', 'FP_1433', 'FP_1434', 'FP_1435', 'FP_1439', 'FP_1442', 'FP_1449', 'FP_1451', 'FP_1458', 'FP_1461', 'FP_1463', 'FP_1465', 'FP_1469', 'FP_1471', 'FP_1477', 'FP_1481', 'FP_1483', 'FP_1484', 'FP_1488', 'FP_1491', 'FP_1492', 'FP_1493', 'FP_1494', 'FP_1495', 'FP_1496', 'FP_1499', 'FP_1501', 'FP_1503', 'FP_1504', 'FP_1505', 'FP_1512', 'FP_1514', 'FP_1515', 'FP_1517', 'FP_1521', 'FP_1523', 'FP_1525', 'FP_1528', 'FP_1529', 'FP_1532', 'FP_1537', 'FP_1538', 'FP_1541', 'FP_1543', 'FP_1546', 'FP_1547', 'FP_1548', 'FP_1549', 'FP_1551', 'FP_1552', 'FP_1553', 'FP_1554', 'FP_1558', 'FP_1560', 'FP_1562', 'FP_1563', 'FP_1568', 'FP_1570', 'FP_1576', 'FP_1577', 'FP_1580', 'FP_1584', 'FP_1585', 'FP_1586', 'FP_1587', 'FP_1593', 'FP_1595', 'FP_1596', 'FP_1600', 'FP_1605', 'FP_1606', 'FP_1608', 'FP_1613', 'FP_1614', 'FP_1615', 'FP_1616', 'FP_1619', 'FP_1622', 'FP_1623', 'FP_1625', 'FP_1626', 'FP_1627', 'FP_1629', 'FP_1632', 'FP_1634', 'FP_1635', 'FP_1636', 'FP_1637', 'FP_1639', 'FP_1641', 'FP_1646', 'FP_1651', 'FP_1657', 'FP_1659', 'FP_1660', 'FP_1662', 'FP_1663', 'FP_1664', 'FP_1667', 'FP_1672', 'FP_1674', 'FP_1676', 'FP_1677', 'FP_1678', 'FP_1679', 'FP_1689', 'FP_1690', 'FP_1694', 'FP_1696', 'FP_1701', 'FP_1702', 'FP_1703', 'FP_1704', 'FP_1705', 'FP_1709', 'FP_1711', 'FP_1727', 'FP_1731', 'FP_1736', 'FP_1738', 'FP_1740', 'FP_1744', 'FP_1746', 'FP_1748', 'FP_1749', 'FP_1759', 'FP_1762', 'FP_1765', 'FP_1766', 'FP_1767', 'FP_1768', 'FP_1769', 'FP_1776', 'FP_1779', 'FP_1780', 'FP_1782', 'FP_1786', 'FP_1790', 'FP_1792', 'FP_1796', 'FP_1797', 'FP_1800', 'FP_1802', 'FP_1805', 'FP_1806', 'FP_1807', 'FP_1808', 'FP_1812', 'FP_1813', 'FP_1815', 'FP_1817', 'FP_1819', 'FP_1821', 'FP_1822', 'FP_1828', 'FP_1829', 'FP_1830', 'FP_1833', 'FP_1835', 'FP_1837', 'FP_1838', 'FP_1841', 'FP_1848', 'FP_1849', 'FP_1851', 'FP_1857', 'FP_1858', 'FP_1864', 'FP_1865', 'FP_1867', 'FP_1868', 'FP_1870', 'FP_1871', 'FP_1874', 'FP_1875', 'FP_1877', 'FP_1881', 'FP_1884', 'FP_1887', 'FP_1889', 'FP_1890', 'FP_1892', 'FP_1894', 'FP_1895', 'FP_1896', 'FP_1897', 'FP_1902', 'FP_1903', 'FP_1905', 'FP_1906', 'FP_1907', 'FP_1910', 'FP_1921', 'FP_1922', 'FP_1925', 'FP_1929', 'FP_1931', 'FP_1932', 'FP_1933', 'FP_1935', 'FP_1937', 'FP_1939', 'FP_1941', 'FP_1944', 'FP_1945', 'FP_1946', 'FP_1959', 'FP_1960', 'FP_1962', 'FP_1965', 'FP_1967', 'FP_1971', 'FP_1988', 'FP_1994', 'FP_2000', 'FP_2001', 'FP_2003', 'FP_2005', 'FP_2008', 'FP_2010', 'FP_2011', 'FP_2012', 'FP_2014', 'FP_2015', 'FP_2016', 'FP_2019', 'FP_2020', 'FP_2021', 'FP_2026', 'FP_2027', 'FP_2028', 'FP_2029', 'FP_2031', 'FP_2035', 'FP_2036', 'FP_2037']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 295): ['FP_155', 'FP_238', 'FP_289', 'FP_377', 'FP_381', 'FP_405', 'FP_414', 'FP_421', 'FP_432', 'FP_442', 'FP_458', 'FP_459', 'FP_468', 'FP_485', 'FP_489', 'FP_504', 'FP_507', 'FP_525', 'FP_531', 'FP_544', 'FP_550', 'FP_551', 'FP_554', 'FP_556', 'FP_559', 'FP_615', 'FP_625', 'FP_644', 'FP_645', 'FP_647', 'FP_662', 'FP_665', 'FP_721', 'FP_731', 'FP_737', 'FP_743', 'FP_750', 'FP_760', 'FP_764', 'FP_772', 'FP_801', 'FP_818', 'FP_819', 'FP_821', 'FP_824', 'FP_826', 'FP_838', 'FP_839', 'FP_849', 'FP_850', 'FP_854', 'FP_868', 'FP_885', 'FP_897', 'FP_905', 'FP_907', 'FP_909', 'FP_916', 'FP_917', 'FP_919', 'FP_920', 'FP_927', 'FP_936', 'FP_949', 'FP_951', 'FP_956', 'FP_972', 'FP_977', 'FP_980', 'FP_982', 'FP_989', 'FP_1000', 'FP_1014', 'FP_1026', 'FP_1027', 'FP_1051', 'FP_1059', 'FP_1062', 'FP_1069', 'FP_1076', 'FP_1085', 'FP_1098', 'FP_1101', 'FP_1108', 'FP_1113', 'FP_1116', 'FP_1118', 'FP_1119', 'FP_1121', 'FP_1123', 'FP_1128', 'FP_1135', 'FP_1138', 'FP_1154', 'FP_1155', 'FP_1156', 'FP_1157', 'FP_1164', 'FP_1168', 'FP_1171', 'FP_1178', 'FP_1187', 'FP_1198', 'FP_1203', 'FP_1210', 'FP_1216', 'FP_1217', 'FP_1227', 'FP_1233', 'FP_1236', 'FP_1237', 'FP_1241', 'FP_1255', 'FP_1265', 'FP_1266', 'FP_1269', 'FP_1270', 'FP_1272', 'FP_1279', 'FP_1285', 'FP_1288', 'FP_1290', 'FP_1294', 'FP_1304', 'FP_1319', 'FP_1321', 'FP_1327', 'FP_1332', 'FP_1335', 'FP_1354', 'FP_1360', 'FP_1362', 'FP_1373', 'FP_1378', 'FP_1379', 'FP_1381', 'FP_1389', 'FP_1393', 'FP_1398', 'FP_1407', 'FP_1408', 'FP_1411', 'FP_1416', 'FP_1419', 'FP_1424', 'FP_1428', 'FP_1431', 'FP_1437', 'FP_1438', 'FP_1443', 'FP_1448', 'FP_1450', 'FP_1455', 'FP_1464', 'FP_1466', 'FP_1467', 'FP_1468', 'FP_1470', 'FP_1472', 'FP_1473', 'FP_1474', 'FP_1475', 'FP_1478', 'FP_1482', 'FP_1486', 'FP_1502', 'FP_1508', 'FP_1509', 'FP_1510', 'FP_1511', 'FP_1513', 'FP_1519', 'FP_1520', 'FP_1524', 'FP_1526', 'FP_1531', 'FP_1533', 'FP_1539', 'FP_1540', 'FP_1555', 'FP_1557', 'FP_1565', 'FP_1567', 'FP_1569', 'FP_1571', 'FP_1572', 'FP_1574', 'FP_1578', 'FP_1579', 'FP_1582', 'FP_1588', 'FP_1590', 'FP_1591', 'FP_1592', 'FP_1601', 'FP_1603', 'FP_1610', 'FP_1611', 'FP_1620', 'FP_1621', 'FP_1630', 'FP_1633', 'FP_1640', 'FP_1648', 'FP_1654', 'FP_1655', 'FP_1656', 'FP_1658', 'FP_1666', 'FP_1671', 'FP_1673', 'FP_1675', 'FP_1681', 'FP_1682', 'FP_1687', 'FP_1700', 'FP_1708', 'FP_1712', 'FP_1720', 'FP_1725', 'FP_1732', 'FP_1735', 'FP_1739', 'FP_1741', 'FP_1743', 'FP_1751', 'FP_1752', 'FP_1753', 'FP_1756', 'FP_1757', 'FP_1764', 'FP_1771', 'FP_1777', 'FP_1778', 'FP_1783', 'FP_1787', 'FP_1789', 'FP_1791', 'FP_1793', 'FP_1801', 'FP_1803', 'FP_1811', 'FP_1831', 'FP_1836', 'FP_1842', 'FP_1844', 'FP_1846', 'FP_1854', 'FP_1861', 'FP_1862', 'FP_1869', 'FP_1872', 'FP_1878', 'FP_1879', 'FP_1882', 'FP_1888', 'FP_1893', 'FP_1898', 'FP_1900', 'FP_1908', 'FP_1912', 'FP_1913', 'FP_1915', 'FP_1923', 'FP_1924', 'FP_1926', 'FP_1938', 'FP_1940', 'FP_1949', 'FP_1950', 'FP_1954', 'FP_1958', 'FP_1963', 'FP_1966', 'FP_1969', 'FP_1974', 'FP_1976', 'FP_1977', 'FP_1979', 'FP_1983', 'FP_1984', 'FP_1996', 'FP_2004', 'FP_2007', 'FP_2009', 'FP_2018', 'FP_2022', 'FP_2030', 'FP_2038', 'FP_2039', 'FP_2040', 'FP_2043', 'FP_2044', 'FP_2045', 'FP_2046']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 295 | ['FP_155', 'FP_238', 'FP_289', 'FP_377', 'FP_381', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 967 | ['NumRotatableBonds', 'RingCount', 'FP_1', 'FP_2', 'FP_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :   2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 965 | ['FP_1', 'FP_2', 'FP_4', 'FP_5', 'FP_6', ...]\n",
      "\t2.2s = Fit runtime\n",
      "\t1355 features in original data used to generate 1355 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.32 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.25s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 38.49s of the 57.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-2.5437\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.93s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 22.31s of the 41.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 932. Best iteration is:\n",
      "\t[927]\tvalid_set's rmse: 3.66763\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-2.6689\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.29s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 5.75s of the 25.00s of remaining time.\n",
      "\t-2.8358\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.76s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2.71s of the 21.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tcatboost/private/libs/options/json_helper.h:41: Can't parse parameter \"iterations\" with value: -38\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 389, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 868, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 350, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 355, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 391, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, num_cpus=num_cpus, num_gpus=num_gpus, **kwargs_fold)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\autogluon\\tabular\\models\\catboost\\catboost_model.py\", line 264, in _fit\n",
      "    self.model.fit(X, **fit_final_kwargs)\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\catboost\\core.py\", line 2395, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"C:\\Users\\Benjamin Gu\\miniconda3\\envs\\mml_comp_chem\\lib\\site-packages\\catboost\\core.py\", line 2321, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6601, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6623, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: catboost/private/libs/options/json_helper.h:41: Can't parse parameter \"iterations\" with value: -38\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1.62s of the 20.87s of remaining time.\n",
      "\t-2.5336\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 0.18s of the 19.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.1s)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 57.75s of the 18.91s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.538, 'LightGBMXT_BAG_L1': 0.462}\n",
      "\t-2.5072\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 18.90s of the 18.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 706. Best iteration is:\n",
      "\t[478]\tvalid_set's rmse: 2.9451\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-2.5997\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.32s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 5.34s of the 5.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 49. Best iteration is:\n",
      "\t[39]\tvalid_set's rmse: 2.14224\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 40. Best iteration is:\n",
      "\t[39]\tvalid_set's rmse: 2.16687\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 33. Best iteration is:\n",
      "\t[33]\tvalid_set's rmse: 2.28211\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 48. Best iteration is:\n",
      "\t[45]\tvalid_set's rmse: 3.09176\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 58. Best iteration is:\n",
      "\t[57]\tvalid_set's rmse: 2.33929\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 66. Best iteration is:\n",
      "\t[66]\tvalid_set's rmse: 2.3038\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 79. Best iteration is:\n",
      "\t[44]\tvalid_set's rmse: 3.01155\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 112. Best iteration is:\n",
      "\t[32]\tvalid_set's rmse: 3.71643\n",
      "\t-2.685\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.94s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 57.75s of the 0.03s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.538, 'LightGBMXT_BAG_L1': 0.462}\n",
      "\t-2.5072\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 59.99s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1326.0 rows/s (69 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Rg\\ds_sub_fit\\sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       LightGBMXT_BAG_L2      -2.054924  -2.599718  root_mean_squared_error        0.319892       0.337037  33.182339                 0.088013                0.041503          13.315639            2       True          6\n",
      "1         LightGBM_BAG_L2      -2.244218  -2.685005  root_mean_squared_error        0.316883       0.337532  24.810145                 0.085004                0.041998           4.943445            2       True          7\n",
      "2       LightGBMXT_BAG_L1      -2.315538  -2.543735  root_mean_squared_error        0.098683       0.035511  15.932355                 0.098683                0.035511          15.932355            1       True          1\n",
      "3     WeightedEnsemble_L3      -2.326350  -2.507207  root_mean_squared_error        0.166371       0.166021  17.105989                 0.006000                0.000000           0.002997            3       True          8\n",
      "4     WeightedEnsemble_L2      -2.326350  -2.507207  root_mean_squared_error        0.166875       0.166021  17.106992                 0.006503                0.000000           0.004000            2       True          5\n",
      "5    ExtraTreesMSE_BAG_L1      -2.374820  -2.533594  root_mean_squared_error        0.061688       0.130509   1.170637                 0.061688                0.130509           1.170637            1       True          4\n",
      "6         LightGBM_BAG_L1      -2.403142  -2.668896  root_mean_squared_error        0.092005       0.036008  16.285932                 0.092005                0.036008          16.285932            1       True          2\n",
      "7  RandomForestMSE_BAG_L1      -2.625911  -2.835798  root_mean_squared_error        0.071508       0.129513   2.763708                 0.071508                0.129513           2.763708            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t61s\t = DyStack   runtime |\t179s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 179s\n",
      "AutoGluon will save models to \"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Rg\"\n",
      "Train Data Rows:    614\n",
      "Train Data Columns: 2438\n",
      "Label Column:       Rg\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16903.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.13 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1305 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 743): ['FP_0', 'FP_3', 'FP_10', 'FP_17', 'FP_18', 'FP_21', 'FP_22', 'FP_23', 'FP_27', 'FP_28', 'FP_30', 'FP_31', 'FP_35', 'FP_36', 'FP_40', 'FP_43', 'FP_44', 'FP_46', 'FP_48', 'FP_49', 'FP_51', 'FP_53', 'FP_55', 'FP_63', 'FP_64', 'FP_66', 'FP_72', 'FP_73', 'FP_76', 'FP_77', 'FP_78', 'FP_79', 'FP_81', 'FP_82', 'FP_85', 'FP_86', 'FP_88', 'FP_93', 'FP_96', 'FP_97', 'FP_98', 'FP_106', 'FP_110', 'FP_111', 'FP_115', 'FP_122', 'FP_123', 'FP_124', 'FP_127', 'FP_131', 'FP_132', 'FP_134', 'FP_139', 'FP_141', 'FP_143', 'FP_146', 'FP_148', 'FP_150', 'FP_153', 'FP_154', 'FP_157', 'FP_159', 'FP_160', 'FP_164', 'FP_166', 'FP_169', 'FP_171', 'FP_176', 'FP_177', 'FP_178', 'FP_179', 'FP_183', 'FP_187', 'FP_188', 'FP_192', 'FP_198', 'FP_199', 'FP_207', 'FP_209', 'FP_211', 'FP_214', 'FP_215', 'FP_217', 'FP_218', 'FP_219', 'FP_220', 'FP_221', 'FP_225', 'FP_228', 'FP_229', 'FP_230', 'FP_236', 'FP_239', 'FP_246', 'FP_254', 'FP_256', 'FP_257', 'FP_258', 'FP_259', 'FP_260', 'FP_261', 'FP_263', 'FP_265', 'FP_267', 'FP_268', 'FP_269', 'FP_275', 'FP_281', 'FP_284', 'FP_285', 'FP_286', 'FP_292', 'FP_293', 'FP_297', 'FP_299', 'FP_300', 'FP_301', 'FP_302', 'FP_309', 'FP_317', 'FP_318', 'FP_320', 'FP_321', 'FP_324', 'FP_326', 'FP_327', 'FP_331', 'FP_332', 'FP_336', 'FP_337', 'FP_338', 'FP_340', 'FP_344', 'FP_346', 'FP_349', 'FP_355', 'FP_356', 'FP_358', 'FP_359', 'FP_363', 'FP_365', 'FP_368', 'FP_369', 'FP_370', 'FP_372', 'FP_373', 'FP_376', 'FP_379', 'FP_384', 'FP_388', 'FP_391', 'FP_393', 'FP_395', 'FP_397', 'FP_398', 'FP_399', 'FP_402', 'FP_408', 'FP_415', 'FP_416', 'FP_417', 'FP_418', 'FP_423', 'FP_426', 'FP_428', 'FP_429', 'FP_431', 'FP_433', 'FP_434', 'FP_435', 'FP_436', 'FP_437', 'FP_439', 'FP_443', 'FP_447', 'FP_449', 'FP_451', 'FP_453', 'FP_454', 'FP_455', 'FP_457', 'FP_462', 'FP_465', 'FP_466', 'FP_467', 'FP_470', 'FP_475', 'FP_476', 'FP_477', 'FP_478', 'FP_480', 'FP_482', 'FP_483', 'FP_491', 'FP_494', 'FP_498', 'FP_505', 'FP_509', 'FP_510', 'FP_511', 'FP_512', 'FP_513', 'FP_515', 'FP_516', 'FP_521', 'FP_524', 'FP_526', 'FP_527', 'FP_529', 'FP_533', 'FP_537', 'FP_542', 'FP_545', 'FP_547', 'FP_548', 'FP_552', 'FP_557', 'FP_558', 'FP_560', 'FP_566', 'FP_568', 'FP_569', 'FP_571', 'FP_572', 'FP_574', 'FP_577', 'FP_578', 'FP_580', 'FP_583', 'FP_586', 'FP_590', 'FP_593', 'FP_595', 'FP_596', 'FP_601', 'FP_605', 'FP_608', 'FP_611', 'FP_612', 'FP_613', 'FP_618', 'FP_623', 'FP_627', 'FP_630', 'FP_631', 'FP_633', 'FP_634', 'FP_635', 'FP_639', 'FP_642', 'FP_643', 'FP_646', 'FP_648', 'FP_649', 'FP_651', 'FP_653', 'FP_657', 'FP_658', 'FP_660', 'FP_661', 'FP_664', 'FP_668', 'FP_669', 'FP_671', 'FP_673', 'FP_676', 'FP_678', 'FP_681', 'FP_683', 'FP_684', 'FP_688', 'FP_696', 'FP_697', 'FP_700', 'FP_704', 'FP_706', 'FP_707', 'FP_708', 'FP_711', 'FP_713', 'FP_719', 'FP_722', 'FP_723', 'FP_724', 'FP_726', 'FP_727', 'FP_733', 'FP_734', 'FP_735', 'FP_738', 'FP_740', 'FP_744', 'FP_751', 'FP_752', 'FP_756', 'FP_758', 'FP_761', 'FP_763', 'FP_765', 'FP_767', 'FP_770', 'FP_771', 'FP_773', 'FP_776', 'FP_779', 'FP_780', 'FP_782', 'FP_784', 'FP_788', 'FP_789', 'FP_792', 'FP_793', 'FP_796', 'FP_797', 'FP_800', 'FP_804', 'FP_806', 'FP_808', 'FP_810', 'FP_812', 'FP_815', 'FP_823', 'FP_830', 'FP_840', 'FP_842', 'FP_846', 'FP_847', 'FP_848', 'FP_851', 'FP_853', 'FP_855', 'FP_856', 'FP_857', 'FP_858', 'FP_859', 'FP_860', 'FP_861', 'FP_865', 'FP_871', 'FP_873', 'FP_874', 'FP_883', 'FP_884', 'FP_889', 'FP_894', 'FP_903', 'FP_904', 'FP_910', 'FP_911', 'FP_912', 'FP_918', 'FP_922', 'FP_923', 'FP_925', 'FP_928', 'FP_929', 'FP_930', 'FP_939', 'FP_942', 'FP_944', 'FP_946', 'FP_947', 'FP_955', 'FP_957', 'FP_958', 'FP_966', 'FP_968', 'FP_970', 'FP_971', 'FP_979', 'FP_983', 'FP_988', 'FP_990', 'FP_991', 'FP_993', 'FP_995', 'FP_996', 'FP_999', 'FP_1001', 'FP_1002', 'FP_1006', 'FP_1007', 'FP_1008', 'FP_1009', 'FP_1010', 'FP_1015', 'FP_1016', 'FP_1018', 'FP_1020', 'FP_1021', 'FP_1023', 'FP_1025', 'FP_1030', 'FP_1033', 'FP_1037', 'FP_1040', 'FP_1042', 'FP_1044', 'FP_1048', 'FP_1050', 'FP_1054', 'FP_1067', 'FP_1068', 'FP_1073', 'FP_1078', 'FP_1079', 'FP_1080', 'FP_1082', 'FP_1092', 'FP_1093', 'FP_1094', 'FP_1095', 'FP_1102', 'FP_1105', 'FP_1106', 'FP_1111', 'FP_1115', 'FP_1117', 'FP_1122', 'FP_1124', 'FP_1125', 'FP_1131', 'FP_1132', 'FP_1134', 'FP_1137', 'FP_1144', 'FP_1146', 'FP_1149', 'FP_1150', 'FP_1151', 'FP_1158', 'FP_1165', 'FP_1167', 'FP_1172', 'FP_1177', 'FP_1179', 'FP_1181', 'FP_1183', 'FP_1186', 'FP_1188', 'FP_1192', 'FP_1193', 'FP_1194', 'FP_1197', 'FP_1200', 'FP_1201', 'FP_1204', 'FP_1205', 'FP_1207', 'FP_1208', 'FP_1214', 'FP_1215', 'FP_1219', 'FP_1221', 'FP_1223', 'FP_1231', 'FP_1234', 'FP_1242', 'FP_1245', 'FP_1246', 'FP_1247', 'FP_1248', 'FP_1250', 'FP_1251', 'FP_1253', 'FP_1254', 'FP_1262', 'FP_1275', 'FP_1277', 'FP_1278', 'FP_1281', 'FP_1289', 'FP_1291', 'FP_1293', 'FP_1296', 'FP_1299', 'FP_1300', 'FP_1301', 'FP_1305', 'FP_1306', 'FP_1307', 'FP_1311', 'FP_1315', 'FP_1316', 'FP_1318', 'FP_1320', 'FP_1324', 'FP_1329', 'FP_1334', 'FP_1336', 'FP_1340', 'FP_1341', 'FP_1342', 'FP_1343', 'FP_1345', 'FP_1348', 'FP_1358', 'FP_1361', 'FP_1364', 'FP_1367', 'FP_1372', 'FP_1374', 'FP_1375', 'FP_1376', 'FP_1387', 'FP_1390', 'FP_1394', 'FP_1395', 'FP_1396', 'FP_1397', 'FP_1401', 'FP_1406', 'FP_1409', 'FP_1412', 'FP_1414', 'FP_1418', 'FP_1421', 'FP_1422', 'FP_1425', 'FP_1426', 'FP_1433', 'FP_1434', 'FP_1435', 'FP_1439', 'FP_1442', 'FP_1449', 'FP_1451', 'FP_1458', 'FP_1463', 'FP_1469', 'FP_1471', 'FP_1477', 'FP_1481', 'FP_1483', 'FP_1484', 'FP_1488', 'FP_1491', 'FP_1492', 'FP_1493', 'FP_1494', 'FP_1495', 'FP_1496', 'FP_1499', 'FP_1501', 'FP_1503', 'FP_1504', 'FP_1505', 'FP_1512', 'FP_1514', 'FP_1515', 'FP_1517', 'FP_1521', 'FP_1523', 'FP_1525', 'FP_1528', 'FP_1529', 'FP_1532', 'FP_1537', 'FP_1538', 'FP_1541', 'FP_1546', 'FP_1547', 'FP_1548', 'FP_1549', 'FP_1552', 'FP_1553', 'FP_1554', 'FP_1558', 'FP_1560', 'FP_1562', 'FP_1563', 'FP_1568', 'FP_1570', 'FP_1576', 'FP_1577', 'FP_1580', 'FP_1584', 'FP_1585', 'FP_1586', 'FP_1587', 'FP_1593', 'FP_1595', 'FP_1596', 'FP_1600', 'FP_1605', 'FP_1606', 'FP_1608', 'FP_1613', 'FP_1614', 'FP_1615', 'FP_1616', 'FP_1619', 'FP_1622', 'FP_1623', 'FP_1625', 'FP_1627', 'FP_1629', 'FP_1632', 'FP_1635', 'FP_1636', 'FP_1637', 'FP_1639', 'FP_1641', 'FP_1646', 'FP_1651', 'FP_1657', 'FP_1659', 'FP_1660', 'FP_1663', 'FP_1664', 'FP_1667', 'FP_1672', 'FP_1674', 'FP_1676', 'FP_1677', 'FP_1678', 'FP_1679', 'FP_1689', 'FP_1690', 'FP_1694', 'FP_1696', 'FP_1701', 'FP_1702', 'FP_1703', 'FP_1704', 'FP_1705', 'FP_1709', 'FP_1711', 'FP_1727', 'FP_1731', 'FP_1736', 'FP_1738', 'FP_1740', 'FP_1744', 'FP_1746', 'FP_1748', 'FP_1749', 'FP_1759', 'FP_1762', 'FP_1765', 'FP_1766', 'FP_1767', 'FP_1768', 'FP_1769', 'FP_1776', 'FP_1779', 'FP_1780', 'FP_1782', 'FP_1786', 'FP_1790', 'FP_1792', 'FP_1796', 'FP_1797', 'FP_1800', 'FP_1802', 'FP_1805', 'FP_1807', 'FP_1808', 'FP_1812', 'FP_1813', 'FP_1815', 'FP_1817', 'FP_1821', 'FP_1822', 'FP_1828', 'FP_1829', 'FP_1830', 'FP_1833', 'FP_1835', 'FP_1837', 'FP_1838', 'FP_1841', 'FP_1848', 'FP_1849', 'FP_1851', 'FP_1857', 'FP_1858', 'FP_1864', 'FP_1865', 'FP_1867', 'FP_1868', 'FP_1870', 'FP_1874', 'FP_1875', 'FP_1877', 'FP_1881', 'FP_1884', 'FP_1887', 'FP_1889', 'FP_1890', 'FP_1892', 'FP_1894', 'FP_1895', 'FP_1896', 'FP_1897', 'FP_1902', 'FP_1903', 'FP_1906', 'FP_1907', 'FP_1910', 'FP_1921', 'FP_1922', 'FP_1925', 'FP_1929', 'FP_1931', 'FP_1932', 'FP_1933', 'FP_1935', 'FP_1937', 'FP_1939', 'FP_1941', 'FP_1944', 'FP_1945', 'FP_1946', 'FP_1959', 'FP_1960', 'FP_1962', 'FP_1965', 'FP_1967', 'FP_1971', 'FP_1988', 'FP_1994', 'FP_2000', 'FP_2001', 'FP_2003', 'FP_2005', 'FP_2008', 'FP_2010', 'FP_2011', 'FP_2012', 'FP_2014', 'FP_2015', 'FP_2016', 'FP_2019', 'FP_2020', 'FP_2021', 'FP_2026', 'FP_2028', 'FP_2029', 'FP_2031', 'FP_2035', 'FP_2036', 'FP_2037']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 272): ['FP_144', 'FP_155', 'FP_238', 'FP_377', 'FP_381', 'FP_405', 'FP_414', 'FP_432', 'FP_442', 'FP_458', 'FP_459', 'FP_468', 'FP_485', 'FP_507', 'FP_525', 'FP_531', 'FP_544', 'FP_550', 'FP_551', 'FP_554', 'FP_602', 'FP_625', 'FP_644', 'FP_647', 'FP_662', 'FP_665', 'FP_712', 'FP_731', 'FP_737', 'FP_743', 'FP_750', 'FP_760', 'FP_764', 'FP_772', 'FP_818', 'FP_819', 'FP_821', 'FP_824', 'FP_826', 'FP_838', 'FP_839', 'FP_850', 'FP_868', 'FP_885', 'FP_897', 'FP_905', 'FP_907', 'FP_909', 'FP_916', 'FP_919', 'FP_920', 'FP_927', 'FP_936', 'FP_949', 'FP_951', 'FP_954', 'FP_956', 'FP_972', 'FP_977', 'FP_980', 'FP_982', 'FP_989', 'FP_1014', 'FP_1026', 'FP_1051', 'FP_1059', 'FP_1062', 'FP_1069', 'FP_1076', 'FP_1085', 'FP_1098', 'FP_1101', 'FP_1108', 'FP_1118', 'FP_1119', 'FP_1121', 'FP_1123', 'FP_1130', 'FP_1135', 'FP_1138', 'FP_1154', 'FP_1155', 'FP_1156', 'FP_1157', 'FP_1164', 'FP_1168', 'FP_1171', 'FP_1178', 'FP_1187', 'FP_1189', 'FP_1191', 'FP_1198', 'FP_1203', 'FP_1210', 'FP_1216', 'FP_1217', 'FP_1227', 'FP_1233', 'FP_1236', 'FP_1241', 'FP_1255', 'FP_1265', 'FP_1266', 'FP_1269', 'FP_1270', 'FP_1272', 'FP_1276', 'FP_1279', 'FP_1285', 'FP_1288', 'FP_1294', 'FP_1304', 'FP_1319', 'FP_1321', 'FP_1327', 'FP_1332', 'FP_1346', 'FP_1360', 'FP_1362', 'FP_1373', 'FP_1378', 'FP_1379', 'FP_1381', 'FP_1389', 'FP_1393', 'FP_1398', 'FP_1407', 'FP_1408', 'FP_1411', 'FP_1416', 'FP_1419', 'FP_1424', 'FP_1427', 'FP_1428', 'FP_1431', 'FP_1437', 'FP_1438', 'FP_1443', 'FP_1448', 'FP_1450', 'FP_1455', 'FP_1461', 'FP_1464', 'FP_1466', 'FP_1467', 'FP_1468', 'FP_1470', 'FP_1472', 'FP_1473', 'FP_1474', 'FP_1475', 'FP_1482', 'FP_1486', 'FP_1502', 'FP_1510', 'FP_1511', 'FP_1513', 'FP_1519', 'FP_1520', 'FP_1524', 'FP_1526', 'FP_1531', 'FP_1539', 'FP_1540', 'FP_1543', 'FP_1551', 'FP_1555', 'FP_1557', 'FP_1565', 'FP_1567', 'FP_1569', 'FP_1571', 'FP_1572', 'FP_1574', 'FP_1578', 'FP_1579', 'FP_1582', 'FP_1588', 'FP_1590', 'FP_1603', 'FP_1610', 'FP_1611', 'FP_1620', 'FP_1621', 'FP_1626', 'FP_1630', 'FP_1633', 'FP_1634', 'FP_1640', 'FP_1648', 'FP_1654', 'FP_1655', 'FP_1656', 'FP_1658', 'FP_1662', 'FP_1666', 'FP_1673', 'FP_1675', 'FP_1681', 'FP_1682', 'FP_1687', 'FP_1700', 'FP_1708', 'FP_1712', 'FP_1720', 'FP_1725', 'FP_1732', 'FP_1735', 'FP_1739', 'FP_1743', 'FP_1751', 'FP_1752', 'FP_1753', 'FP_1756', 'FP_1757', 'FP_1764', 'FP_1771', 'FP_1777', 'FP_1783', 'FP_1787', 'FP_1789', 'FP_1791', 'FP_1793', 'FP_1801', 'FP_1803', 'FP_1806', 'FP_1811', 'FP_1831', 'FP_1836', 'FP_1842', 'FP_1844', 'FP_1854', 'FP_1861', 'FP_1862', 'FP_1869', 'FP_1871', 'FP_1872', 'FP_1882', 'FP_1888', 'FP_1898', 'FP_1905', 'FP_1908', 'FP_1912', 'FP_1913', 'FP_1923', 'FP_1924', 'FP_1926', 'FP_1938', 'FP_1940', 'FP_1949', 'FP_1950', 'FP_1954', 'FP_1958', 'FP_1963', 'FP_1969', 'FP_1974', 'FP_1976', 'FP_1977', 'FP_1979', 'FP_1983', 'FP_1984', 'FP_2007', 'FP_2009', 'FP_2018', 'FP_2022', 'FP_2030', 'FP_2038', 'FP_2039', 'FP_2040', 'FP_2044', 'FP_2045', 'FP_2046']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 272 | ['FP_144', 'FP_155', 'FP_238', 'FP_377', 'FP_381', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])   : 1035 | ['NumRotatableBonds', 'RingCount', 'FP_1', 'FP_2', 'FP_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  388 | ['MolLogP', 'TPSA', 'MolWt', 'BertzCT', 'ChemBERTa_0', ...]\n",
      "\t\t('int', [])       :    2 | ['NumRotatableBonds', 'RingCount']\n",
      "\t\t('int', ['bool']) : 1033 | ['FP_1', 'FP_2', 'FP_4', 'FP_5', 'FP_6', ...]\n",
      "\t2.1s = Fit runtime\n",
      "\t1423 features in original data used to generate 1423 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.53 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 118.00s of the 177.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 2.03908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 2.58338\n",
      "[2000]\tvalid_set's rmse: 2.58096\n",
      "[3000]\tvalid_set's rmse: 2.58056\n",
      "[4000]\tvalid_set's rmse: 2.58048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4401. Best iteration is:\n",
      "\t[4396]\tvalid_set's rmse: 2.58047\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-2.435\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.8s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 83.87s of the 142.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-2.5755\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.25s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 63.32s of the 122.36s of remaining time.\n",
      "\t-2.7719\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.19s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 59.87s of the 118.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-2.5213\t = Validation score   (-root_mean_squared_error)\n",
      "\t42.29s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 17.21s of the 76.25s of remaining time.\n",
      "\t-2.4945\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 15.39s of the 74.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 14.86s of the 73.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\t-2.7027\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.33s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1.27s of the 60.30s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 0.68s of the 59.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.0s)\n",
      "\tTime limit exceeded... Skipping LightGBMLarge_BAG_L1.\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 0.24s of the 59.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.1s)\n",
      "\tTime limit exceeded... Skipping CatBoost_r177_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 177.04s of the 58.74s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.609, 'ExtraTreesMSE_BAG_L1': 0.261, 'CatBoost_BAG_L1': 0.13}\n",
      "\t-2.4188\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 58.73s of the 58.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 2.96797\n",
      "[2000]\tvalid_set's rmse: 2.96182\n",
      "[3000]\tvalid_set's rmse: 2.9608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3113. Best iteration is:\n",
      "\t[3069]\tvalid_set's rmse: 2.96077\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-2.4298\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.78s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 35.65s of the 35.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 3.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1211. Best iteration is:\n",
      "\t[1033]\tvalid_set's rmse: 3.03488\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-2.4209\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.98s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 16.38s of the 16.36s of remaining time.\n",
      "\t-2.4637\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.13s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 12.94s of the 12.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-2.5495\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.47s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 177.04s of the 0.89s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.5, 'ExtraTreesMSE_BAG_L1': 0.25, 'RandomForestMSE_BAG_L2': 0.15, 'CatBoost_BAG_L1': 0.1}\n",
      "\t-2.3825\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 178.28s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 201.2 rows/s (77 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\Benjamin Gu\\Desktop\\MML_project\\ag_models\\Rg\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training Complete. 30 Minutes Well Spent!\n",
      "üìÑ Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SEGMENT 4: AutoGluon Multi-Target Training\n",
    "# =============================================================================\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Define target columns (Properties to predict)\n",
    "TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "# 2. Define metadata columns to exclude from training\n",
    "# id: Just an identifier\n",
    "# SMILES/Sanitized_SMILES: Raw strings (converted to features already)\n",
    "# Scaffold: Used for splitting, not direct training\n",
    "METADATA_COLS = ['id', 'SMILES', 'Sanitized_SMILES', 'Scaffold']\n",
    "\n",
    "# ‚è±Ô∏è Time Allocation Strategy (Total ~30 mins)\n",
    "time_allocation = {\n",
    "  'FFV': 900,      # 15 mins: High priority\n",
    "  'Tg': 180,       # 3 mins: Smaller dataset\n",
    "  'Tc': 240,       # 4 mins\n",
    "  'Density': 240,  # 4 mins\n",
    "  'Rg': 240        # 4 mins\n",
    "}\n",
    "\n",
    "# Setup output container\n",
    "submission = pd.DataFrame({'id': full_test['id']})\n",
    "MODEL_ROOT = 'ag_models'\n",
    "\n",
    "for target in TARGETS:\n",
    "  # Get time limit for current target, default to 300s\n",
    "  current_time_limit = time_allocation.get(target, 300)\n",
    "  \n",
    "  print(f\"\\nüéØ Training for Target: {target} | ‚è≥ Time Limit: {current_time_limit}s\")\n",
    "  \n",
    "  # 1. Filter valid data (remove rows where target is NaN)\n",
    "  train_data = full_train[full_train[target].notna()].copy()\n",
    "  \n",
    "  if len(train_data) == 0:\n",
    "      print(f\"   ‚ö†Ô∏è No training data for {target}, skipping...\")\n",
    "      continue\n",
    "\n",
    "  # 2. Drop metadata and other target columns to prevent leakage\n",
    "  other_targets = [t for t in TARGETS if t != target]\n",
    "  drop_cols = METADATA_COLS + other_targets\n",
    "  train_data = train_data.drop(columns=drop_cols, errors='ignore')\n",
    "  \n",
    "  # 3. Train Model\n",
    "  save_path = os.path.join(MODEL_ROOT, target)\n",
    "  if os.path.exists(save_path): shutil.rmtree(save_path)\n",
    "  \n",
    "  predictor = TabularPredictor(label=target, path=save_path, problem_type='regression')\n",
    "  \n",
    "  # Note: If time is tight, 'best_quality' might downgrade automatically.\n",
    "  # We aim for 'best_quality' to utilize Bagging/Stacking.\n",
    "  predictor.fit(\n",
    "      train_data, \n",
    "      presets='best_quality',  \n",
    "      time_limit=current_time_limit,   \n",
    "      ag_args_fit={'num_gpus': 1} # Comment this out if GPU is not available\n",
    "  )\n",
    "  \n",
    "  # 4. Predict\n",
    "  test_features = full_test.drop(columns=METADATA_COLS, errors='ignore')\n",
    "  submission[target] = predictor.predict(test_features)\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete.\")\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"üìÑ Saved submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22966346-dd7f-4b8a-9a45-3a742ff9a7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MML CompChem)",
   "language": "python",
   "name": "mml_comp_chem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
